{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This lecture is quite unusual because it contains only practical part. On the previous lecture, we talked about the theory behind feedforward neural networks, their structure, and the algorithm called backpropagation. During this lecture, we are going to improve Python implementation of a neural network, which was forged at the previous lecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all, we are going to import class NeuralNet, which was written at the previous lecture, using following structure (library itself is stored in github folder \"Modules\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import neural_net_lib as net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we are going to import all needed libraries for this part of the lecture. They are divided into two categories: crucial and cosmetic, which are needed for simplification of code or visualization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#crucial\n",
    "import random\n",
    "import pickle\n",
    "#cosmetic\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After importing libraries, we are defining the main task - classification of numbers by their look. In other words, we have an image of a number (28x28) and a neural network has to answer which number it is. Hardcoding rules manually using if-statements is an extraordinarily hard task, if not impossible. That is why we are going to use a flexible neural network to do this work for us.\n",
    "\n",
    "As we have learned in the previous lecture, a neural network needs to have examples to learn on. During this lecture, we are going to use MNIST database which main part consists of 55000 images and, accordingly, labels (numbers from 0 to 9). This database can be easily uploaded using library Tensorflow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object mnist contains all images in mnist.train.images and all labels in mnist.train.labels. Each image can be accessed using an index. For example mnist.train.images[i], where i is from 0 to 54999, is an array of numbers with length 784 (28x28=784).\n",
    "\n",
    "All labels are represented as *one-hot vectors*, which means that each label is a list of zeros with an exception of one position, which is equal to one. For example 2 can be encoded as [0,0,1,0,0,0,0,0,0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n"
     ]
    }
   ],
   "source": [
    "print(mnist.train.labels[0]) #this is seven"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using matplotlib we can plot any image from database:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAEyCAYAAACbGke8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAD+9JREFUeJzt3WuIXHWax/Hfz3vwEiJpQ3Bce1Zl\n2bi4iXRk0XWMjgb1TQzqYl4kvTASXyhEsMUQggZkQZcxs6JrYtRgBG/jJSqizogG3YFFUpGorc1o\niMlMTCdpMWK8wKA++6JLaLPdqX/XpavryfcDTVWdfup/npPT/PKvU+dUOSIEAFkc0e4GAKCZCDUA\nqRBqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqEGoBUjprIlU2fPj26u7sncpUAktiyZcvnEdFVq25C\nQ627u1uVSmUiVwkgCds7S+oaevlp+3Lbf7a9zfbyRsYCgGaoO9RsHynpvyVdIWmWpEW2ZzWrMQCo\nRyMztfMkbYuI7RHxN0lPSVrQnLYAoD6NhNqpkv464vGu6rKfsb3UdsV2ZWhoqIHVAUBtjYSaR1n2\n/z6cLSLWRURPRPR0ddV84wIAGtJIqO2SdNqIx7+QtLuxdgCgMY2E2mZJZ9n+pe1jJF0n6aXmtAUA\n9an7PLWI+N72TZL+IOlISesj4sOmdQYAdWjo5NuIeEXSK03qBQAaxrWfAFIh1ACkQqgBSIVQA5AK\noQYgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqEGoBUCDUAqRBqAFIh1ACk\nQqgBSIVQA5AKoQYgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5DKUe1uABivb775pmbNrbfe\nWjTW2rVri+rmzZtXVLdx48aaNVOnTi0aC/VhpgYgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQ\nA5AKoQYgFa4oQMsdOHCgqO7ll18uqlu9enXNmi1bthSNZbuo7q233iqqW7ZsWc2aRx99tGgs1Keh\nULO9Q9IBST9I+j4ieprRFADUqxkztYsj4vMmjAMADeOYGoBUGg21kPRH21tsLx2twPZS2xXblaGh\noQZXBwCH1mioXRAR50q6QtKNtn91cEFErIuInojo6erqanB1AHBoDYVaROyu3u6TtFHSec1oCgDq\nVXeo2T7e9ok/3Zc0X1J/sxoDgHo08u7nDEkbq+f5HCXpiYh4rSldAUCd6g61iNgu6Z+b2AuS6uvr\nK6p7+OGHW9xJ63V3d7e7hcMep3QASIVQA5AKoQYgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIWP\n80ZDNm7cWLPm6aefnoBOWuv8888vqrvtttta3AlqYaYGIBVCDUAqhBqAVAg1AKkQagBSIdQApEKo\nAUiFUAOQCqEGIBWuKMCo9u3bV1S3ePHimjXfffdd0Vhnn312Ud2OHTtq1nz99ddFY/X29hbVrV69\nuqhuypQpRXVoHWZqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqEGoBUCDUAqXDyLUa1adOmorpvv/22\nZs2xxx5bNFZfX19R3ZIlS4rqOt2ePXuK6h577LGaNcuXL2+0nZ8ZHBwsqpsxY0ZT11uCmRqAVAg1\nAKkQagBSIdQApEKoAUiFUAOQCqEGIBVCDUAqhBqAVLiiAKO68847i+ps16y59tpri8bKcKXA/v37\na9asWLGiaKwHH3ywqK5kH5x00klFY61ataqoburUqUV17cBMDUAqNUPN9nrb+2z3j1h2su3XbX9S\nvZ3W2jYBoEzJTO1RSZcftGy5pDci4ixJb1QfA0Db1Qy1iHhb0hcHLV4gaUP1/gZJVzW5LwCoS73H\n1GZExKAkVW9PGavQ9lLbFduVoaGhOlcHAGVa/kZBRKyLiJ6I6Onq6mr16gAc5uoNtb22Z0pS9bbs\n67wBoMXqDbWXJPVW7/dKerE57QBAY0pO6XhS0v9K+gfbu2z/RtJdki6z/Ymky6qPAaDtal5REBGL\nxvjVr5vcCybAM888U1T30UcfFdWVnM2+ePHiorHa4cCBA0V1Tz31VFHd/fffX7Omv7+/Zs14XHLJ\nJTVr7r777qKxzj333EbbaTuuKACQCqEGIBVCDUAqhBqAVAg1AKkQagBSIdQApEKoAUiFUAOQCt9R\ncJjZuXNnu1uYEGvXri2qu/fee4vqPv7440ba+ZnS72yYP39+Ud3ChQtr1kybdvh8ODUzNQCpEGoA\nUiHUAKRCqAFIhVADkAqhBiAVQg1AKoQagFQ4+RaTRunHXC9YsKBmzaefflo0VsnHkUvS6aefXlR3\n++2316xZsmRJ0VhHHMGcox78qwFIhVADkAqhBiAVQg1AKoQagFQINQCpEGoAUiHUAKRCqAFIhSsK\nMKqIaNpYK1euLKrbvHlz09Y5Y8aMorpbbrmlqO6aa64pquvu7i6qQ+swUwOQCqEGIBVCDUAqhBqA\nVAg1AKkQagBSIdQApEKoAUiFUAOQClcUYFSln91fUlepVJq6zt7e3po1fX19RWPNmjWrqA6do+ZM\nzfZ62/ts949Ytsr2Z7a3Vn+ubG2bAFCm5OXno5IuH2X57yJidvXnlea2BQD1qRlqEfG2pC8moBcA\naFgjbxTcZPv96svTaWMV2V5qu2K7MjQ01MDqAKC2ekNtjaQzJM2WNCjpnrEKI2JdRPRERE9XV1ed\nqwOAMnWFWkTsjYgfIuJHSQ9JOq+5bQFAfeoKNdszRzxcKKl/rFoAmEg1z1Oz/aSkeZKm294l6Q5J\n82zPlhSSdki6oYU9AkCxmqEWEYtGWfxIC3pBA3bv3l1U98QTT7S4k/pdf/31RXX33XdfzZpjjjmm\n0XbQobhMCkAqhBqAVAg1AKkQagBSIdQApEKoAUiFUAOQCqEGIBVCDUAqfJx3Byi5WmDBggVFY733\n3nuNttMyF198cVEdVwvgUJipAUiFUAOQCqEGIBVCDUAqhBqAVAg1AKkQagBSIdQApEKoAUiFKwra\nqL+/7Eu4Lrroopo1X375ZdFYc+fOLarr6ekpqluzZk1RXYnXXnutqO66665r2jqRDzM1AKkQagBS\nIdQApEKoAUiFUAOQCqEGIBVCDUAqhBqAVAg1AKlwRUEL7Nmzp6iu9Mz4/fv316yZPn160Vh33HFH\nUd2cOXOK6h544IGiuhIR0bSxcPhipgYgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5AKJ9+2\nwOrVq4vqBgYGiurOOeecmjWbNm0qGmvatGlFdW+++WZRne2iuokeC4cvZmoAUqkZarZPs73J9oDt\nD20vqy4/2fbrtj+p3pZNAQCghUpmat9LuiUi/lHSv0i60fYsScslvRERZ0l6o/oYANqqZqhFxGBE\nvFu9f0DSgKRTJS2QtKFatkHSVa1qEgBKjeuYmu1uSXMkvSNpRkQMSsPBJ+mUMZ6z1HbFdmVoaKix\nbgGghuJQs32CpOck3RwRX5U+LyLWRURPRPR0dXXV0yMAFCsKNdtHazjQHo+I56uL99qeWf39TEn7\nWtMiAJQreffTkh6RNBARI0/AeklSb/V+r6QXm98eAIxPycm3F0haLOkD21ury1ZIukvS723/RtJf\nJF3bmhYBoFzNUIuIP0ka61TvXze3HYxm9+7dNWteffXVorGeffbZoroXXnihqK6ZVwFcffXVTRsL\nhy+uKACQCqEGIBVCDUAqhBqAVAg1AKkQagBSIdQApEKoAUiFUAOQiiNiwlbW09MTlUplwtbXLtu2\nbSuqu+KKK4rqtm/f3kg7dSn9u7jwwgtr1vT19RWNdemllxbVTZkypagOudjeEhE9teqYqQFIhVAD\nkAqhBiAVQg1AKoQagFQINQCpEGoAUiHUAKRS8h0FGKczzzyzqK70I7jnz59fs2bnzp1FY82dO7eo\nbuXKlUV1JSfMHnfccUVjAc3ATA1AKoQagFQINQCpEGoAUiHUAKRCqAFIhVADkAqhBiAVQg1AKlxR\n0EalVx604+O8gU7FTA1AKoQagFQINQCpEGoAUiHUAKRCqAFIhVADkAqhBiAVQg1AKoQagFRqhprt\n02xvsj1g+0Pby6rLV9n+zPbW6s+VrW8XAA6t5NrP7yXdEhHv2j5R0hbbr1d/97uI+G3r2gOA8akZ\nahExKGmwev+A7QFJp7a6MQCox7iOqdnuljRH0jvVRTfZft/2etvTmtwbAIxbcajZPkHSc5Jujoiv\nJK2RdIak2Rqeyd0zxvOW2q7YrgwNDTWhZQAYW1Go2T5aw4H2eEQ8L0kRsTcifoiIHyU9JOm80Z4b\nEesioicierq6uprVNwCMquTdT0t6RNJARKwesXzmiLKFkvqb3x4AjE/Ju58XSFos6QPbW6vLVkha\nZHu2pJC0Q9INLekQAMah5N3PP0nyKL96pfntAEBjuKIAQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQ\nA5AKoQYgFUINQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCqEGoBUCDUAqTgiJm5l\n9pCknQctni7p8wlrovk6vX+p87eh0/uXOn8bJqL/0yOi5vdsTmiojdqAXYmInrY20YBO71/q/G3o\n9P6lzt+GydQ/Lz8BpEKoAUhlMoTaunY30KBO71/q/G3o9P6lzt+GSdN/24+pAUAzTYaZGgA0DaEG\nIJW2hZrty23/2fY228vb1UcjbO+w/YHtrbYr7e6nhO31tvfZ7h+x7GTbr9v+pHo7rZ09HsoY/a+y\n/Vl1P2y1fWU7ezwU26fZ3mR7wPaHtpdVl3fSPhhrGybFfmjLMTXbR0r6WNJlknZJ2ixpUUR8NOHN\nNMD2Dkk9EdExJ03a/pWkryU9FhH/VF32n5K+iIi7qv/BTIuI29rZ51jG6H+VpK8j4rft7K2E7ZmS\nZkbEu7ZPlLRF0lWS/l2dsw/G2oZ/0yTYD+2aqZ0naVtEbI+Iv0l6StKCNvVyWImItyV9cdDiBZI2\nVO9v0PAf6KQ0Rv8dIyIGI+Ld6v0DkgYknarO2gdjbcOk0K5QO1XSX0c83qVJ9I8yDiHpj7a32F7a\n7mYaMCMiBqXhP1hJp7S5n3rcZPv96svTSfvSbSTb3ZLmSHpHHboPDtoGaRLsh3aFmkdZ1onnllwQ\nEedKukLSjdWXRph4aySdIWm2pEFJ97S3ndpsnyDpOUk3R8RX7e6nHqNsw6TYD+0KtV2SThvx+BeS\ndrepl7pFxO7q7T5JGzX8sroT7a0eJ/npeMm+NvczLhGxNyJ+iIgfJT2kSb4fbB+t4TB4PCKery7u\nqH0w2jZMlv3QrlDbLOks27+0fYyk6yS91KZe6mL7+OpBUtk+XtJ8Sf2Hftak9ZKk3ur9XkkvtrGX\ncfspDKoWahLvB9uW9IikgYhYPeJXHbMPxtqGybIf2nZFQfXt3v+SdKSk9RHxH21ppE62/17DszNJ\nOkrSE52wDbaflDRPwx8Vs1fSHZJekPR7SX8n6S+Sro2ISXkwfoz+52n4JU9I2iHphp+OT002tv9V\n0v9I+kDSj9XFKzR8TKpT9sFY27BIk2A/cJkUgFS4ogBAKoQagFQINQCpEGoAUiHUAKRCqAFIhVAD\nkMr/AfAG4JGPsQ91AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22375eab00>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=random.randint(0,54999)\n",
    "fig=plt.figure(figsize=(5,5))\n",
    "plt.imshow(mnist.train.images[i].reshape((28,28)),cmap=plt.get_cmap('binary'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing hyperparametres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are considering a neural network with one input, one hidden, and one output layers. Obviously, a number of input and output neurons must be equal to the number of inputs and outputs, respectively. Still, we have left with a question: \"How many hidden neurons should we have?\" Professionals suggest following formula:\n",
    "$$N_{hid}=\\frac{N_{data}}{\\alpha (N_{inp}+N_{out})}$$\n",
    "where $N_{data}$ is the number of x-y data pairs; $N_{inp}$, $N_{hid}$, $N_{out}$ - number of neurons in input, hidden, and output layers, respectively.\n",
    "\n",
    "Parameter $\\alpha$ should be between 2-10. If it's too high then a number of hidden neurons is too small, and a neural network has low \"brain capacity\", which means that it is going to be too hard to learn patterns in data. On the other hand, if $\\alpha$ is too low neural network has more hidden neurons, more weights, and more flexibility. This can lead to an **overfitting**, which means that neural network \"memorized\" data so good that it becomes bad outside of training data.\n",
    "\n",
    "To counter overfitting it is common practice to slice all data into training and testing. After finishing training neural network on training data, you can calculate loss on testing data and compare neural networks by this performance. Data is usually split in ratio 70-90/30-10, which is ratio training/testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this formula, we can calculate that optimal number of hidden layers is about 16-32. Using results, we are going to assign neural network with 784 input, 32 hidden, and 10 output neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn=net.NeuralNet(784,32,10,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Actual training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can remember that we are using gradient descent for updating weights in a neural network. This algorithm has one sensible parameter, which is called learning rate. If learning rate is too low, training is too slow. On the other hand, if it is too high, weight updates are chaotic or even exponentially unstable. For our uses, the optimal value can be found from the experiment. In reality, finding all hyperparameters is a hot topic in machine learning, which can be solved using machine learning itself!\n",
    "\n",
    "For current data and neural network, I am using learning rate $\\eta=0.1$. Maybe you can find a better value! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loss_list=[] \n",
    "#each 100 iterations we print out loss for single x-y example\n",
    "#Usually, it's better to calculate mean loss, because whole process is stohastic.\n",
    "#However, this can be computationally expensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[  1.50112657e-03   1.01999454e-03   6.57035145e-02   9.56526333e-01\n",
      "   1.66354060e-03   2.75874705e-02   5.83659314e-05   2.77025069e-03\n",
      "   3.49147614e-02   2.24472970e-04]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "0.00410040493377\n",
      "100\n",
      "[  5.72771196e-03   2.45061338e-03   4.55393812e-02   9.52557355e-05\n",
      "   1.34081112e-02   1.06593618e-02   9.81943771e-01   6.22755670e-05\n",
      "   1.75513526e-03   3.93574730e-04]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.00136766131247\n",
      "200\n",
      "[  3.29333290e-04   4.15749382e-03   4.60546831e-02   5.69114152e-03\n",
      "   1.07991138e-01   5.17296469e-04   7.38092499e-05   8.19981591e-01\n",
      "   1.65162773e-03   7.83304453e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "0.0231519435158\n",
      "300\n",
      "[  2.80630979e-03   1.79501378e-03   8.72058020e-01   4.32748853e-03\n",
      "   6.87712592e-02   4.86482971e-04   6.44944250e-02   3.79200127e-04\n",
      "   1.65631993e-02   7.45620083e-04]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.0127816338776\n",
      "400\n",
      "[  4.83175109e-04   9.28247810e-01   1.47378236e-02   1.70126536e-02\n",
      "   4.66756443e-05   7.46389092e-04   2.88071551e-03   6.70041076e-03\n",
      "   7.02197352e-02   2.76034306e-02]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00570087900472\n",
      "500\n",
      "[  8.24732858e-01   1.85188372e-04   3.54913554e-02   1.44593071e-02\n",
      "   3.90942319e-06   3.70532950e-03   5.26192675e-04   9.20980076e-02\n",
      "   5.03658529e-04   2.80630858e-04]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.0203418474542\n",
      "600\n",
      "[  1.84202470e-01   5.81087033e-04   7.77383784e-01   1.38714627e-02\n",
      "   1.16215915e-05   4.79097585e-03   7.71845902e-03   5.44995814e-03\n",
      "   1.08933910e-02   1.66253547e-03]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.0419574724647\n",
      "700\n",
      "[  4.53527195e-03   1.23520459e-02   7.00786477e-03   9.94196007e-01\n",
      "   2.29070325e-05   2.53389106e-02   2.61349780e-04   9.19998195e-04\n",
      "   1.44912343e-02   6.36340982e-03]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "0.00057470135197\n",
      "800\n",
      "[  1.09891641e-02   3.64763707e-03   9.31712722e-01   3.62612302e-03\n",
      "   8.33912573e-05   5.15652641e-04   5.14009017e-03   2.57091812e-04\n",
      "   1.20893692e-01   4.62417355e-04]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00972631313925\n",
      "900\n",
      "[  2.38084563e-03   7.32365002e-03   4.16571283e-02   8.14663775e-05\n",
      "   1.75585276e-02   2.99329040e-03   9.91047256e-01   1.91490261e-04\n",
      "   1.78962157e-03   1.94235470e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.00109952635724\n",
      "1000\n",
      "[  5.59357584e-03   1.64263977e-03   4.42440413e-02   1.78113609e-04\n",
      "   1.63863809e-02   2.22053617e-03   9.86103309e-01   7.14689820e-04\n",
      "   5.14478547e-04   3.73052340e-04]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.00122951508725\n",
      "1100\n",
      "[  3.40863208e-03   1.23392317e-02   8.68827495e-03   9.95639312e-01\n",
      "   1.22023129e-04   9.80781969e-03   1.83681292e-04   8.44874514e-04\n",
      "   4.86926487e-03   1.51559835e-03]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "0.000190669838673\n",
      "1200\n",
      "[  4.79423939e-02   4.15389777e-04   1.90855764e-02   1.79812446e-04\n",
      "   1.73943962e-03   4.19989948e-04   5.94086284e-03   5.50040769e-03\n",
      "   2.40931150e-02   5.89973684e-01]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.656604980163\n",
      "1300\n",
      "[  1.04658845e-03   4.57893328e-03   3.31265564e-03   7.35238506e-04\n",
      "   1.17356740e-02   5.82872492e-02   9.67255131e-01   1.95972199e-04\n",
      "   5.07213587e-03   8.01121297e-04]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.00233366946239\n",
      "1400\n",
      "[  4.48080589e-04   4.81258428e-02   4.54924366e-02   1.30493324e-04\n",
      "   1.30194894e-02   2.61501215e-03   9.60281849e-01   1.75639374e-03\n",
      "   4.33862855e-02   7.09972594e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.00403780700768\n",
      "1500\n",
      "[  6.38333548e-04   1.34944679e-04   5.96574116e-04   5.35745584e-04\n",
      "   9.81176548e-01   3.47480830e-02   1.40421463e-02   8.29872007e-03\n",
      "   9.94395532e-03   2.13120953e-02]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.00119097924051\n",
      "1600\n",
      "[  1.50048976e-03   2.78377072e-03   3.54897574e-02   1.73092534e-03\n",
      "   6.88612410e-03   5.11439699e-02   1.00111776e-01   3.17261361e-04\n",
      "   8.48466228e-01   4.57055315e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "0.0184707433627\n",
      "1700\n",
      "[  2.21115020e-04   9.25354724e-01   1.26928656e-02   4.33517703e-03\n",
      "   1.12191919e-03   1.58394793e-03   3.21229952e-02   5.03441642e-04\n",
      "   3.28006760e-02   3.82705237e-03]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00393915365191\n",
      "1800\n",
      "[  6.41930650e-03   2.76684043e-03   5.77067519e-02   7.55133609e-03\n",
      "   1.42482044e-03   2.09462487e-02   1.47553938e-03   2.46208875e-04\n",
      "   9.58287605e-01   2.23647841e-02]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "0.00305953775434\n",
      "1900\n",
      "[  3.93904202e-02   3.23083289e-03   1.74706451e-02   4.17502617e-04\n",
      "   3.14686792e-03   7.00938473e-02   9.42327904e-01   7.82887750e-05\n",
      "   1.25243132e-02   5.57081990e-04]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.00513686847913\n",
      "2000\n",
      "[  7.88556195e-05   2.68465671e-04   4.62063241e-03   1.22665348e-03\n",
      "   9.57376195e-01   1.82922964e-03   1.24212755e-02   1.45829304e-02\n",
      "   3.17793396e-02   1.04342267e-01]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.00705362660319\n",
      "2100\n",
      "[  3.01978454e-03   1.33766593e-04   7.37820663e-04   3.85914109e-04\n",
      "   7.28150872e-01   6.80197338e-03   1.16308726e-02   3.13732660e-02\n",
      "   1.98508277e-02   2.23641736e-01]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.0627436429684\n",
      "2200\n",
      "[  4.15026846e-03   3.17250839e-02   8.11453814e-02   4.43105489e-04\n",
      "   2.50902884e-02   5.34083362e-03   9.26361004e-01   4.42819708e-05\n",
      "   1.59431000e-02   2.25619613e-04]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.00697172950269\n",
      "2300\n",
      "[  5.09559107e-01   8.21458960e-05   8.50855382e-04   2.14086111e-03\n",
      "   7.07019156e-04   1.46470107e-01   2.24529281e-02   1.46793009e-03\n",
      "   1.08300816e-02   1.73525350e-02]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "0.494547133211\n",
      "2400\n",
      "[  5.41220079e-03   1.66557180e-04   5.25237374e-02   2.34382984e-03\n",
      "   9.11638421e-01   9.11669227e-04   2.67737023e-02   7.81787519e-04\n",
      "   3.22420532e-03   1.16349554e-02]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.005732683017\n",
      "2500\n",
      "[  1.10733778e-04   8.97889297e-01   2.10538188e-02   2.12526233e-02\n",
      "   1.60844459e-04   2.77187920e-03   6.34382691e-03   1.88946601e-03\n",
      "   1.50656188e-02   4.46817097e-03]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00581000306371\n",
      "2600\n",
      "[  6.60910610e-05   1.15528148e-03   5.75269512e-03   3.52411856e-04\n",
      "   5.08467276e-01   4.05274146e-03   7.81700549e-03   2.38127511e-02\n",
      "   5.41808634e-02   4.87189046e-01]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "0.262564372583\n",
      "2700\n",
      "[  3.60983576e-04   1.81960837e-01   1.86880297e-02   5.20108235e-04\n",
      "   1.44459937e-01   2.66680220e-02   1.23527800e-01   5.55714575e-04\n",
      "   4.33512732e-01   8.77405213e-04]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "0.602448343531\n",
      "2800\n",
      "[  9.65298565e-01   3.45582136e-05   5.23120768e-03   1.39220564e-03\n",
      "   3.88054221e-05   4.76151595e-02   2.18558908e-03   3.09561940e-03\n",
      "   1.05060996e-03   3.31398217e-05]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00175808201041\n",
      "2900\n",
      "[  2.67000175e-03   1.18316009e-02   9.93305474e-01   1.73451836e-02\n",
      "   2.27063304e-04   3.68037110e-04   1.08487097e-02   3.38783300e-04\n",
      "   1.87699715e-03   2.47690773e-04]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.000307184260046\n",
      "3000\n",
      "[  1.73691656e-04   9.08840669e-01   3.68575379e-02   9.07129370e-03\n",
      "   1.07474021e-03   1.58697261e-03   3.38749321e-03   5.61832595e-03\n",
      "   2.26165958e-02   1.33842638e-03]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00515541812914\n",
      "3100\n",
      "[  8.75060345e-01   1.64264804e-04   3.48633569e-01   9.06053360e-03\n",
      "   1.22043072e-05   4.66971396e-03   7.60053703e-04   8.05002666e-03\n",
      "   3.21002778e-02   1.71257204e-04]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.0691775236037\n",
      "3200\n",
      "[  1.17758756e-04   3.48962906e-03   6.11999800e-03   4.42931553e-03\n",
      "   1.20023358e-02   6.40460207e-04   3.67895020e-04   9.67884262e-01\n",
      "   1.15940795e-03   9.62439602e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "0.000669630017567\n",
      "3300\n",
      "[  1.69830587e-04   6.24298419e-04   6.18124483e-04   1.46267787e-03\n",
      "   1.06769081e-01   1.00736428e-03   1.98254605e-03   8.22415673e-03\n",
      "   3.31634815e-02   9.71140137e-01]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "0.00670393344308\n",
      "3400\n",
      "[  1.00934557e-05   9.92056689e-01   1.67751916e-03   4.32658758e-02\n",
      "   6.13483550e-04   1.10630197e-02   2.35926265e-02   6.77787750e-03\n",
      "   6.75701478e-03   9.14260600e-03]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.0013962046425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3500\n",
      "[  3.03287302e-05   9.71885681e-01   6.07473929e-03   2.17902107e-02\n",
      "   1.46791330e-04   4.35375969e-03   4.61183899e-03   7.33099884e-03\n",
      "   1.29011815e-02   3.23674469e-03]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.000786518976256\n",
      "3600\n",
      "[  2.72193227e-04   8.98767826e-05   1.13663223e-03   6.19556449e-04\n",
      "   9.72981264e-01   8.82231490e-03   5.56612149e-03   3.54710876e-03\n",
      "   5.17544992e-02   3.56082303e-02]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.00239982061526\n",
      "3700\n",
      "[  1.27243039e-04   5.34109191e-03   1.32808487e-03   1.53344996e-03\n",
      "   5.58490371e-02   4.08846703e-02   9.78574896e-03   1.09915422e-03\n",
      "   9.78843315e-01   3.44503555e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "0.00268988627725\n",
      "3800\n",
      "[  6.73630019e-04   9.58635654e-05   6.89724432e-04   3.89873828e-05\n",
      "   9.46746913e-01   2.66334162e-02   7.62287360e-03   3.24584218e-02\n",
      "   3.29832551e-02   4.29441691e-02]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.00379496221645\n",
      "3900\n",
      "[  1.24197689e-04   5.78053435e-02   1.25597509e-03   1.52120680e-03\n",
      "   7.40149011e-01   9.28912916e-02   1.21007494e-02   2.15773028e-02\n",
      "   3.24948567e-02   1.29462463e-02]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.0406661111925\n",
      "4000\n",
      "[  5.11186079e-03   7.05052553e-04   8.72487672e-01   9.94273696e-03\n",
      "   8.73070443e-03   1.06836647e-03   3.94465752e-03   1.37140896e-04\n",
      "   1.28780619e-02   1.90175399e-03]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00832364346857\n",
      "4100\n",
      "[  4.21902718e-02   5.06623619e-04   9.12489572e-01   1.32462154e-02\n",
      "   2.59994888e-05   9.39585684e-04   2.76971457e-04   4.60676784e-03\n",
      "   2.95367343e-02   1.61618140e-03]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00525551304883\n",
      "4200\n",
      "[  1.15881055e-02   3.92145841e-05   9.26647734e-05   5.66897430e-02\n",
      "   6.04998863e-03   6.71558962e-01   1.80417692e-04   1.37431066e-02\n",
      "   7.77869010e-03   9.89430515e-02]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "0.0606486399747\n",
      "4300\n",
      "[  3.61049561e-02   2.66607793e-04   2.57171025e-04   2.56718989e-04\n",
      "   1.74026924e-02   9.70396610e-01   8.19192588e-03   9.03750072e-03\n",
      "   2.05191888e-02   4.26844152e-02]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "0.00243738292791\n",
      "4400\n",
      "[  3.10805914e-03   1.18625646e-01   2.26564769e-03   5.91999182e-03\n",
      "   3.15525764e-04   1.59419402e-02   1.41053256e-03   1.25974669e-03\n",
      "   7.71120446e-01   1.71527022e-02]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "0.0335298851266\n",
      "4500\n",
      "[  5.00436383e-04   6.47863818e-05   1.48864091e-04   4.27503745e-04\n",
      "   9.71764310e-01   7.96671688e-02   9.00875920e-03   1.67054365e-02\n",
      "   4.88030873e-02   1.33848838e-02]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.00503284866928\n",
      "4600\n",
      "[  2.51406411e-05   9.35899895e-01   2.75065086e-02   1.45026061e-02\n",
      "   2.04763486e-04   9.22210140e-04   1.03252568e-02   3.43502757e-02\n",
      "   8.67489858e-03   3.03146226e-03]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00322382303678\n",
      "4700\n",
      "[  5.88614726e-05   9.49251997e-01   3.01627312e-02   3.07896010e-03\n",
      "   6.83245523e-03   1.51878321e-03   3.61767439e-02   3.69998134e-03\n",
      "   1.31513009e-02   9.95258760e-04]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00252000835307\n",
      "4800\n",
      "[  1.68167604e-05   9.39878893e-01   2.08601064e-03   3.26725349e-02\n",
      "   7.99333234e-05   9.13816029e-03   2.39476927e-03   4.79572084e-02\n",
      "   2.33552058e-03   2.44013763e-02]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00383820836643\n",
      "4900\n",
      "[  6.91651399e-04   1.30932242e-02   9.87439734e-01   1.83244128e-02\n",
      "   2.03841871e-04   1.16383584e-04   1.37405952e-02   1.27951590e-02\n",
      "   1.64097962e-03   1.47544149e-04]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.000510372506959\n",
      "5000\n",
      "[  3.78306525e-03   5.61594352e-03   1.50566013e-01   2.66018765e-03\n",
      "   2.77093479e-03   4.10261251e-03   1.79682133e-03   1.75184838e-04\n",
      "   9.88698509e-01   5.88916669e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "0.011456613054\n",
      "5100\n",
      "[  3.73193629e-04   1.75861332e-02   4.08809116e-02   4.50477771e-04\n",
      "   2.07758693e-02   4.26758079e-03   9.70220777e-01   1.02441047e-03\n",
      "   2.44939069e-02   7.13656996e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.00198472291269\n",
      "5200\n",
      "[  1.65214219e-04   2.29847850e-03   2.65598359e-03   2.97678708e-04\n",
      "   9.17866361e-01   5.95157144e-03   1.01253921e-01   1.49892465e-02\n",
      "   2.83792791e-02   9.05180804e-02]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.013134874746\n",
      "5300\n",
      "[  1.88132269e-02   4.77038201e-04   8.48043783e-04   2.59664430e-02\n",
      "   6.88947225e-04   8.87148859e-01   4.38903094e-02   7.90624124e-03\n",
      "   8.38745377e-03   8.84413856e-06]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "0.00791210622243\n",
      "5400\n",
      "[  2.14036367e-05   9.85507379e-01   5.28313479e-04   1.89669961e-02\n",
      "   2.06506813e-04   4.11892635e-02   5.43988802e-03   4.81915400e-03\n",
      "   3.50932016e-02   1.24704093e-02]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00185326059404\n",
      "5500\n",
      "[  1.64391686e-03   1.94023734e-02   3.91235972e-03   9.49807527e-01\n",
      "   1.60427556e-04   5.35901395e-02   1.47130091e-06   3.22117310e-02\n",
      "   1.35998753e-02   5.64052423e-04]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "0.00350427231048\n",
      "5600\n",
      "[  2.93593732e-02   7.55028706e-04   8.17464679e-01   1.33542031e-02\n",
      "   1.41510347e-03   5.97372214e-04   3.39521663e-03   1.91539723e-04\n",
      "   1.81429283e-02   2.02394900e-02]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.017556373647\n",
      "5700\n",
      "[  1.51336439e-02   1.20319416e-03   9.92273059e-01   8.93316390e-03\n",
      "   3.05588777e-04   5.20881831e-04   3.54878548e-03   5.29768178e-04\n",
      "   2.20383931e-02   2.34848330e-04]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.000434483520316\n",
      "5800\n",
      "[  3.31935556e-03   5.44864638e-03   1.30149393e-04   1.77901861e-03\n",
      "   1.18608331e-02   9.48413045e-01   2.06141219e-03   6.53492507e-02\n",
      "   1.37031698e-03   2.34745617e-05]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "0.00356121667003\n",
      "5900\n",
      "[  3.19090008e-03   6.24774774e-03   4.33603478e-03   9.85170195e-01\n",
      "   5.08921102e-04   6.57200019e-03   1.64641643e-05   8.65482604e-03\n",
      "   4.46246201e-02   9.12451461e-03]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "0.00124045524084\n",
      "6000\n",
      "[  3.53482567e-01   2.22761901e-04   2.84700727e-04   3.34812979e-03\n",
      "   3.74111745e-03   2.70244224e-01   1.09147808e-02   4.85827341e-03\n",
      "   1.39693750e-01   6.52505058e-05]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.597917362082\n",
      "6100\n",
      "[  3.21943708e-04   1.75835722e-04   1.84157064e-03   2.92536867e-03\n",
      "   8.39793847e-01   1.12616509e-02   1.56121512e-03   2.45699889e-02\n",
      "   1.21074876e-02   3.84397838e-02]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.0140176249769\n",
      "6200\n",
      "[  7.32751267e-02   2.73314448e-03   6.02100313e-03   7.24455961e-04\n",
      "   8.39132355e-03   8.35941432e-01   6.81961301e-03   1.55734890e-03\n",
      "   2.39658155e-02   2.50019744e-04]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "0.0165112373857\n",
      "6300\n",
      "[  2.08590198e-04   1.23879036e-01   1.08898827e-02   2.95256250e-03\n",
      "   4.56268851e-02   2.96740444e-03   5.94694687e-01   1.53079547e-04\n",
      "   1.36509952e-01   2.21129779e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.100238130824\n",
      "6400\n",
      "[  3.20704306e-02   3.19523360e-05   7.31442689e-06   5.90742686e-03\n",
      "   3.57313280e-03   7.37611927e-02   1.51839989e-04   7.91656805e-01\n",
      "   3.69759657e-04   7.88780289e-02]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "0.0280728412149\n",
      "6500\n",
      "[  3.50239787e-04   4.50813708e-02   9.11780324e-03   5.83767162e-04\n",
      "   5.13302655e-04   3.94711182e-03   1.88741885e-04   9.37799068e-01\n",
      "   3.88489192e-03   5.48746320e-02]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "0.00451354010159\n",
      "6600\n",
      "[  2.25492593e-04   2.37615340e-04   1.98897251e-03   4.18617021e-05\n",
      "   9.82005365e-01   1.39530218e-02   2.61079573e-02   8.08234097e-03\n",
      "   1.01604329e-02   2.84466466e-02]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.00109097727956\n",
      "6700\n",
      "[  7.96288631e-05   1.31782986e-02   7.42477680e-04   8.21106043e-03\n",
      "   9.11244784e-03   5.91679878e-03   4.32499915e-04   9.32417940e-02\n",
      "   3.89848741e-03   8.43282781e-01]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "0.0168146979518\n",
      "6800\n",
      "[  2.39285649e-02   6.53272634e-03   1.93411743e-01   9.27920988e-01\n",
      "   3.79942000e-05   3.52176090e-03   1.12990202e-03   6.26455451e-04\n",
      "   9.42078770e-05   6.13862335e-05]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "0.949443192767\n",
      "6900\n",
      "[  9.83579938e-01   4.78621010e-05   2.80880826e-02   6.01258561e-03\n",
      "   8.06878395e-05   1.27835786e-02   4.64829005e-03   6.33619828e-03\n",
      "   7.49737457e-03   4.96327484e-04]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.000688174838293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000\n",
      "[  2.43470912e-04   1.22956282e-02   2.83556770e-05   5.43433938e-02\n",
      "   1.94740084e-01   9.55192712e-03   1.13823131e-03   1.13526885e-01\n",
      "   1.35693982e-02   3.84418658e-01]\n",
      "[ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.]\n",
      "0.406245350628\n",
      "7100\n",
      "[  8.88651977e-01   2.65633242e-05   3.80937349e-02   1.19444325e-02\n",
      "   2.38510799e-04   5.03560992e-02   4.77223069e-02   5.02281803e-03\n",
      "   1.81428029e-02   3.43842062e-03]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00958580493801\n",
      "7200\n",
      "[  5.85822466e-04   6.46298241e-04   2.82559823e-05   1.60032174e-03\n",
      "   4.55115686e-02   1.36992691e-02   7.24259222e-04   4.50848192e-02\n",
      "   1.16104526e-02   9.25751038e-01]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "0.00497158603209\n",
      "7300\n",
      "[  8.66505269e-05   2.17648539e-03   3.12877135e-04   1.34522478e-02\n",
      "   3.23337125e-02   3.31755538e-03   1.07177196e-04   9.34026926e-02\n",
      "   1.26409640e-02   7.96253065e-01]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "0.0258194812718\n",
      "7400\n",
      "[  2.06092260e-04   2.89998409e-04   2.96531859e-04   2.41399513e-03\n",
      "   4.84490806e-02   1.69786440e-03   1.09311431e-03   1.92894665e-02\n",
      "   1.81285968e-02   9.73572594e-01]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "0.00187828511817\n",
      "7500\n",
      "[  2.30291512e-02   3.03080108e-04   8.06143853e-04   4.62203563e-02\n",
      "   5.74452256e-04   9.46138793e-01   1.80226115e-03   4.75726503e-04\n",
      "   4.66627513e-02   1.61371744e-03]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "0.00387612769732\n",
      "7600\n",
      "[  5.56765563e-04   3.97662071e-03   8.64346104e-01   3.84535488e-03\n",
      "   5.48182909e-03   5.89509766e-04   1.24541790e-01   1.65707621e-03\n",
      "   9.62690106e-03   2.40023923e-04]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.0170347130072\n",
      "7700\n",
      "[  9.30299672e-01   4.02881399e-05   6.95299492e-02   2.06452043e-02\n",
      "   1.86055683e-05   8.11690937e-03   4.19182800e-02   2.25828531e-02\n",
      "   2.76942248e-03   1.01617862e-03]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.0062302450031\n",
      "7800\n",
      "[  4.66578508e-03   6.93136568e-03   6.04975810e-03   9.94899779e-01\n",
      "   3.60419654e-05   1.79234998e-02   1.77183240e-04   3.19421801e-03\n",
      "   7.73672140e-03   5.49181294e-03]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "0.000276964820701\n",
      "7900\n",
      "[  2.46447342e-05   9.77316496e-01   2.45332739e-04   3.73380036e-02\n",
      "   1.25246210e-04   2.71091443e-02   7.44694677e-03   8.02348086e-03\n",
      "   2.27015010e-02   2.30633511e-01]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.0282353289696\n",
      "8000\n",
      "[  1.35425634e-01   5.47559218e-03   2.00637299e-02   1.26783985e-01\n",
      "   1.57853391e-06   1.00522240e-02   1.64926634e-04   1.59205239e-01\n",
      "   1.78913617e-03   2.63507109e-04]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.394723100619\n",
      "8100\n",
      "[  1.34801107e-02   4.18566151e-05   3.75828903e-05   4.56599683e-04\n",
      "   1.88657903e-01   1.99396230e-02   2.33993189e-02   6.44218243e-02\n",
      "   1.36774078e-02   4.93159855e-01]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "0.148971510688\n",
      "8200\n",
      "[  3.18002518e-03   5.33790835e-03   4.14072499e-02   1.52539264e-04\n",
      "   7.71882296e-03   2.81910668e-03   9.70734093e-01   1.99135665e-04\n",
      "   8.17862097e-03   1.98085534e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.00137403180688\n",
      "8300\n",
      "[  1.08283297e-03   7.75355492e-05   6.13626576e-04   4.42351759e-03\n",
      "   2.33013913e-02   1.39094971e-02   5.75406877e-04   2.49343570e-02\n",
      "   6.93572248e-02   9.79084364e-01]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "0.00331374662756\n",
      "8400\n",
      "[  1.90454271e-03   4.67686250e-03   9.89936796e-01   1.66216535e-02\n",
      "   8.83697386e-05   1.05939483e-03   1.71480355e-03   4.92201828e-03\n",
      "   1.04632064e-02   3.14369034e-04]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.000270461108046\n",
      "8500\n",
      "[  3.15451391e-05   2.47397202e-02   6.62099167e-04   1.92256103e-03\n",
      "   4.63154551e-05   5.75803891e-03   2.44522309e-04   9.43428125e-01\n",
      "   1.59510814e-02   2.38281412e-02]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "0.00233600034486\n",
      "8600\n",
      "[  1.96719790e-03   1.88538293e-02   2.07114153e-03   9.78891586e-01\n",
      "   1.09449340e-04   4.12217070e-02   3.07134844e-05   5.33763540e-04\n",
      "   2.19243187e-02   3.78286738e-04]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "0.00149476865077\n",
      "8700\n",
      "[  4.20497990e-03   1.19093687e-02   2.92523167e-02   4.08117676e-04\n",
      "   3.89098328e-03   1.13504978e-02   9.80297292e-01   1.24927883e-04\n",
      "   1.41343722e-02   8.03929403e-04]\n",
      "[ 0.  0.  0.  0.  0.  0.  1.  0.  0.  0.]\n",
      "0.00087399608152\n",
      "8800\n",
      "[  5.47979334e-05   2.66793455e-03   1.13293305e-03   7.29394124e-04\n",
      "   1.73566550e-01   8.78618757e-03   7.94479354e-03   1.55786917e-01\n",
      "   3.15486980e-03   7.65437846e-01]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "0.0547867606616\n",
      "8900\n",
      "[  1.50385852e-02   4.13176672e-04   1.17503267e-02   1.23598699e-01\n",
      "   3.28783982e-04   2.99913376e-02   1.10624881e-04   9.10549078e-04\n",
      "   9.21673205e-01   2.25140691e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  1.  0.]\n",
      "0.0113408119112\n",
      "9000\n",
      "[  6.67429084e-04   2.21766643e-02   1.18378891e-02   7.59694636e-04\n",
      "   3.39261819e-04   3.70636358e-03   6.46319636e-05   5.49242545e-01\n",
      "   6.24267451e-02   1.76979975e-03]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "0.10386466672\n",
      "9100\n",
      "[  7.38918594e-06   7.77308589e-01   1.77776549e-03   4.43910462e-02\n",
      "   1.83206051e-02   2.64595282e-02   4.54277365e-02   1.03291398e-03\n",
      "   6.15393885e-02   4.81495361e-03]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.0292379838351\n",
      "9200\n",
      "[  2.10387418e-03   1.37808136e-04   6.20501641e-03   6.50070803e-04\n",
      "   5.07143811e-02   1.39165357e-04   1.70392798e-03   4.23980721e-02\n",
      "   6.69866125e-03   9.42896380e-01]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  1.]\n",
      "0.00386076662918\n",
      "9300\n",
      "[  3.41286530e-04   3.56569211e-04   1.45152509e-03   1.81270763e-02\n",
      "   9.67189357e-03   2.50159682e-03   4.54299052e-04   1.45640979e-02\n",
      "   2.08985479e-02   9.28467307e-01]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "0.913438600369\n",
      "9400\n",
      "[  1.49496540e-04   4.26791004e-02   9.11119324e-01   1.33072332e-02\n",
      "   1.45744684e-04   3.46558742e-05   1.92656953e-02   7.32925521e-02\n",
      "   4.65990618e-04   4.05555123e-04]\n",
      "[ 0.  0.  1.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00782087716941\n",
      "9500\n",
      "[  1.03015181e-01   1.29196498e-03   4.54243892e-04   5.33596264e-01\n",
      "   1.04397514e-03   6.82509441e-01   2.77192577e-04   8.69871021e-03\n",
      "   5.03261471e-03   9.72754214e-06]\n",
      "[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.]\n",
      "0.347033873064\n",
      "9600\n",
      "[  9.16009278e-01   2.79359365e-05   2.71188277e-02   3.38038451e-03\n",
      "   1.18074631e-05   2.13633184e-02   3.63001593e-02   2.57014799e-03\n",
      "   9.29936073e-04   2.16666561e-05]\n",
      "[ 1.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.00479143199241\n",
      "9700\n",
      "[  7.41639113e-05   5.18291552e-01   8.13044085e-03   6.62388197e-03\n",
      "   3.74603713e-04   3.21767584e-03   7.33990775e-04   2.55654878e-01\n",
      "   2.36346709e-02   1.35459914e-02]\n",
      "[ 0.  1.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "0.149132777519\n",
      "9800\n",
      "[  3.46233715e-02   1.24402975e-05   6.50798068e-06   1.72410994e-03\n",
      "   1.24957453e-02   2.23757859e-02   1.86857147e-04   8.00805914e-01\n",
      "   2.61288225e-03   2.17493234e-01]\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  1.  0.  0.]\n",
      "0.0444235114476\n",
      "9900\n",
      "[  8.55431226e-03   1.27839569e-02   1.10444695e-03   1.20370493e-02\n",
      "   1.36313441e-02   8.63895873e-01   5.84375023e-03   1.12620041e-02\n",
      "   1.93846221e-02   4.97421773e-05]\n",
      "[ 0.  0.  0.  0.  0.  1.  0.  0.  0.  0.]\n",
      "0.00981480564971\n",
      "711.4614291191101\n"
     ]
    }
   ],
   "source": [
    "t0=time.time() #tracking down time of a training\n",
    "for i in range(10000):\n",
    "    t=random.randint(0,len(mnist.train.images)-1) #choosing random image from the database\n",
    "    if (i%100==0):\n",
    "        print(i)\n",
    "        prediction=np.array(nn.y(mnist.train.images[t]))\n",
    "        real=mnist.train.labels[t]\n",
    "        loss=0.5*np.sum((prediction-real)**2) #calculating loss\n",
    "        loss_list.append(loss)\n",
    "        print(prediction)\n",
    "        print(real)\n",
    "        print(loss)\n",
    "        \n",
    "    x_train=[mnist.train.images[t]] #creating x,y data\n",
    "    y_train=[mnist.train.labels[t]]\n",
    "    nn.train(x_train,y_train,0.1) #training\n",
    "    \n",
    "t1=time.time()\n",
    "print(t1-t0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#you can get a loss plot\n",
    "plt.plot(loss_list)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running this code 4 consecutive times, we have trained a neural network for an hour and gotten sufficient results. Now we can check results by showing image and printing calculated result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAE/CAYAAAAnhFRiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22218719b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=random.randint(0,54999)\n",
    "fig=plt.figure(figsize=(5,5))\n",
    "plt.imshow(mnist.train.images[i].reshape((28,28)),cmap=plt.get_cmap('binary'))\n",
    "digit=np.argmax(nn.y(mnist.train.images[i])) #argmax chooses max value of calculated y-value\n",
    "plt.title('This is number {0}'.format(digit))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After shutting down Jupyter notebook, values of all variables are lost. Considering that training can last for hours and days, we need to find a way of preserving results. Neural network's behavior is determined by its hyperparameters and weights. Hyperparameters are easy to remember but weights can be huge tables, which are hard for printing out. For example, the current neural network consists out of 25408 weights and this number can be much bigger.\n",
    "\n",
    "Luckily, we can use library pickle which can dump pretty much each object into a binary code and then extract this object from a file with .p extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#We are gathering both table of weights into one dictionary and dump it into .p file\n",
    "model_1={}\n",
    "model_1['wa']=nn.wa\n",
    "model_1['wb']=nn.wb\n",
    "pickle.dump(model_1,open(\"model_1.p\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, after reopening Jupyter notebook, we can load a dictionary of table weights using following code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_1=pickle.load(open(\"model_1.p\",\"rb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can assign already trained weights to a new neural network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.wa=model_1['wa']\n",
    "nn.wb=model_1['wb']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding how backpropagation works for feedforward neural network can already be quite a challenge. The complexity of architecture is going to only increase, which makes computing gradients even harder. That is why we are not going to manually derive formulas and later implement them in Python. Instead, we are going to explore how data propagates forward and trust backpropagation calculation to already created libraries.\n",
    "\n",
    "The first library, which we are going to use is Keras because it has one of the simplest syntaxes and is perfectly suited for beginners in machine learning.\n",
    "\n",
    "Speed is one more advantage of Keras over our Python library. Keras is build upon Tensorflow, which uses numpy, which uses special algorithms encoded in C/C++ for actions with weight tables. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feedforward neural network in Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most basic model in Keras is Sequential, which consists of linearly stacked layers. In other words, data travels through each layer of neurons once in one direction. Feedforward neural network perfectly fits for this model, and it consists from similar layers which are called Dense in Keras (because each neuron of the previous layer is connected with each neuron of the next layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "55000/55000 [==============================] - 2s 42us/step - loss: 0.2022 - acc: 0.1120\n",
      "Epoch 2/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.1257 - acc: 0.1231\n",
      "Epoch 3/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.1042 - acc: 0.1487\n",
      "Epoch 4/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0968 - acc: 0.1798\n",
      "Epoch 5/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0934 - acc: 0.1982\n",
      "Epoch 6/200\n",
      "55000/55000 [==============================] - 2s 27us/step - loss: 0.0916 - acc: 0.2207\n",
      "Epoch 7/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0905 - acc: 0.2502\n",
      "Epoch 8/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0898 - acc: 0.2784\n",
      "Epoch 9/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0893 - acc: 0.3019\n",
      "Epoch 10/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0890 - acc: 0.3185\n",
      "Epoch 11/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0887 - acc: 0.3288\n",
      "Epoch 12/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0884 - acc: 0.3355\n",
      "Epoch 13/200\n",
      "55000/55000 [==============================] - 2s 33us/step - loss: 0.0882 - acc: 0.3397\n",
      "Epoch 14/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0880 - acc: 0.3431\n",
      "Epoch 15/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0878 - acc: 0.3456\n",
      "Epoch 16/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0876 - acc: 0.3481\n",
      "Epoch 17/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0874 - acc: 0.3495\n",
      "Epoch 18/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0873 - acc: 0.3523\n",
      "Epoch 19/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0871 - acc: 0.3537\n",
      "Epoch 20/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0869 - acc: 0.3563\n",
      "Epoch 21/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0868 - acc: 0.3585\n",
      "Epoch 22/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0866 - acc: 0.3608\n",
      "Epoch 23/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0864 - acc: 0.3653\n",
      "Epoch 24/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0862 - acc: 0.3672\n",
      "Epoch 25/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0861 - acc: 0.3707\n",
      "Epoch 26/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0859 - acc: 0.3736\n",
      "Epoch 27/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0857 - acc: 0.3775\n",
      "Epoch 28/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0855 - acc: 0.3806\n",
      "Epoch 29/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0853 - acc: 0.3834\n",
      "Epoch 30/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0851 - acc: 0.3868\n",
      "Epoch 31/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0849 - acc: 0.3903\n",
      "Epoch 32/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0847 - acc: 0.3945\n",
      "Epoch 33/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0845 - acc: 0.3990\n",
      "Epoch 34/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0842 - acc: 0.4025\n",
      "Epoch 35/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0840 - acc: 0.4080\n",
      "Epoch 36/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0838 - acc: 0.4116\n",
      "Epoch 37/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0836 - acc: 0.4180\n",
      "Epoch 38/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0833 - acc: 0.4221\n",
      "Epoch 39/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0831 - acc: 0.4274\n",
      "Epoch 40/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0829 - acc: 0.4318\n",
      "Epoch 41/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0826 - acc: 0.4373\n",
      "Epoch 42/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0824 - acc: 0.4428\n",
      "Epoch 43/200\n",
      "55000/55000 [==============================] - 2s 33us/step - loss: 0.0821 - acc: 0.4483\n",
      "Epoch 44/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0819 - acc: 0.4534\n",
      "Epoch 45/200\n",
      "55000/55000 [==============================] - 2s 33us/step - loss: 0.0816 - acc: 0.4592\n",
      "Epoch 46/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0814 - acc: 0.4645\n",
      "Epoch 47/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0811 - acc: 0.4693\n",
      "Epoch 48/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0809 - acc: 0.4743\n",
      "Epoch 49/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0806 - acc: 0.4791\n",
      "Epoch 50/200\n",
      "55000/55000 [==============================] - 2s 36us/step - loss: 0.0804 - acc: 0.4847\n",
      "Epoch 51/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0801 - acc: 0.4901\n",
      "Epoch 52/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0798 - acc: 0.4941\n",
      "Epoch 53/200\n",
      "55000/55000 [==============================] - 2s 27us/step - loss: 0.0796 - acc: 0.4993\n",
      "Epoch 54/200\n",
      "55000/55000 [==============================] - 2s 33us/step - loss: 0.0793 - acc: 0.5055\n",
      "Epoch 55/200\n",
      "55000/55000 [==============================] - 2s 33us/step - loss: 0.0790 - acc: 0.5103\n",
      "Epoch 56/200\n",
      "55000/55000 [==============================] - 2s 33us/step - loss: 0.0788 - acc: 0.5161\n",
      "Epoch 57/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0785 - acc: 0.5217\n",
      "Epoch 58/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0782 - acc: 0.5258\n",
      "Epoch 59/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0780 - acc: 0.5305\n",
      "Epoch 60/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0777 - acc: 0.5362\n",
      "Epoch 61/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0774 - acc: 0.5410\n",
      "Epoch 62/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0771 - acc: 0.5457\n",
      "Epoch 63/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0769 - acc: 0.5506\n",
      "Epoch 64/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0766 - acc: 0.5552\n",
      "Epoch 65/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0763 - acc: 0.5599\n",
      "Epoch 66/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0760 - acc: 0.5641: \n",
      "Epoch 67/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0758 - acc: 0.5681\n",
      "Epoch 68/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0755 - acc: 0.5723\n",
      "Epoch 69/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0752 - acc: 0.5768\n",
      "Epoch 70/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0749 - acc: 0.5807\n",
      "Epoch 71/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0746 - acc: 0.5843\n",
      "Epoch 72/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0744 - acc: 0.5888\n",
      "Epoch 73/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0741 - acc: 0.5923\n",
      "Epoch 74/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0738 - acc: 0.5959\n",
      "Epoch 75/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0735 - acc: 0.5998\n",
      "Epoch 76/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0732 - acc: 0.6034\n",
      "Epoch 77/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0730 - acc: 0.6074\n",
      "Epoch 78/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0727 - acc: 0.6107\n",
      "Epoch 79/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0724 - acc: 0.6141\n",
      "Epoch 80/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0721 - acc: 0.6174\n",
      "Epoch 81/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000/55000 [==============================] - 1s 26us/step - loss: 0.0718 - acc: 0.6206\n",
      "Epoch 82/200\n",
      "55000/55000 [==============================] - 1s 26us/step - loss: 0.0716 - acc: 0.6238\n",
      "Epoch 83/200\n",
      "55000/55000 [==============================] - 1s 26us/step - loss: 0.0713 - acc: 0.6269\n",
      "Epoch 84/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0710 - acc: 0.6303\n",
      "Epoch 85/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0707 - acc: 0.6341\n",
      "Epoch 86/200\n",
      "55000/55000 [==============================] - 1s 27us/step - loss: 0.0704 - acc: 0.6372\n",
      "Epoch 87/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0701 - acc: 0.6403\n",
      "Epoch 88/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0699 - acc: 0.6428\n",
      "Epoch 89/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0696 - acc: 0.6460\n",
      "Epoch 90/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0693 - acc: 0.6490\n",
      "Epoch 91/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0690 - acc: 0.6523\n",
      "Epoch 92/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0688 - acc: 0.6553\n",
      "Epoch 93/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0685 - acc: 0.6583\n",
      "Epoch 94/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0682 - acc: 0.6610\n",
      "Epoch 95/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0679 - acc: 0.6640\n",
      "Epoch 96/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0676 - acc: 0.6663\n",
      "Epoch 97/200\n",
      "55000/55000 [==============================] - 2s 33us/step - loss: 0.0674 - acc: 0.6692\n",
      "Epoch 98/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0671 - acc: 0.6719\n",
      "Epoch 99/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0668 - acc: 0.6745\n",
      "Epoch 100/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0665 - acc: 0.6760\n",
      "Epoch 101/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0663 - acc: 0.6789\n",
      "Epoch 102/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0660 - acc: 0.6815\n",
      "Epoch 103/200\n",
      "55000/55000 [==============================] - 1s 27us/step - loss: 0.0657 - acc: 0.6838\n",
      "Epoch 104/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0655 - acc: 0.6858\n",
      "Epoch 105/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0652 - acc: 0.6881\n",
      "Epoch 106/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0649 - acc: 0.6898\n",
      "Epoch 107/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0647 - acc: 0.6917\n",
      "Epoch 108/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0644 - acc: 0.6935\n",
      "Epoch 109/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0641 - acc: 0.6955\n",
      "Epoch 110/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0639 - acc: 0.6973\n",
      "Epoch 111/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0636 - acc: 0.6993\n",
      "Epoch 112/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0633 - acc: 0.7013\n",
      "Epoch 113/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0631 - acc: 0.7032\n",
      "Epoch 114/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0628 - acc: 0.7048\n",
      "Epoch 115/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0626 - acc: 0.7063\n",
      "Epoch 116/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0623 - acc: 0.7078\n",
      "Epoch 117/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0621 - acc: 0.7101\n",
      "Epoch 118/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0618 - acc: 0.7118\n",
      "Epoch 119/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0616 - acc: 0.7130\n",
      "Epoch 120/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0613 - acc: 0.7147\n",
      "Epoch 121/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0611 - acc: 0.7163\n",
      "Epoch 122/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0608 - acc: 0.7179\n",
      "Epoch 123/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0606 - acc: 0.7198\n",
      "Epoch 124/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0603 - acc: 0.7215\n",
      "Epoch 125/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0601 - acc: 0.7228\n",
      "Epoch 126/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0599 - acc: 0.7244: 1\n",
      "Epoch 127/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0596 - acc: 0.7256\n",
      "Epoch 128/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0594 - acc: 0.7270\n",
      "Epoch 129/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0592 - acc: 0.7286\n",
      "Epoch 130/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0589 - acc: 0.7302\n",
      "Epoch 131/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0587 - acc: 0.7314\n",
      "Epoch 132/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0585 - acc: 0.7331\n",
      "Epoch 133/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0583 - acc: 0.7343\n",
      "Epoch 134/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0580 - acc: 0.7357\n",
      "Epoch 135/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0578 - acc: 0.7370\n",
      "Epoch 136/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0576 - acc: 0.7380\n",
      "Epoch 137/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0574 - acc: 0.7393\n",
      "Epoch 138/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0571 - acc: 0.7404\n",
      "Epoch 139/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0569 - acc: 0.7420\n",
      "Epoch 140/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0567 - acc: 0.7432\n",
      "Epoch 141/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0565 - acc: 0.7442\n",
      "Epoch 142/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0563 - acc: 0.7457\n",
      "Epoch 143/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0561 - acc: 0.7470\n",
      "Epoch 144/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0559 - acc: 0.7482\n",
      "Epoch 145/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0557 - acc: 0.7498\n",
      "Epoch 146/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0555 - acc: 0.7506\n",
      "Epoch 147/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0553 - acc: 0.7518\n",
      "Epoch 148/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0551 - acc: 0.7528\n",
      "Epoch 149/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0549 - acc: 0.7542\n",
      "Epoch 150/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0547 - acc: 0.7553\n",
      "Epoch 151/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0545 - acc: 0.7564\n",
      "Epoch 152/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0543 - acc: 0.7573\n",
      "Epoch 153/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0541 - acc: 0.7582\n",
      "Epoch 154/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0539 - acc: 0.7589\n",
      "Epoch 155/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0537 - acc: 0.7601\n",
      "Epoch 156/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0535 - acc: 0.7611\n",
      "Epoch 157/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0533 - acc: 0.7623\n",
      "Epoch 158/200\n",
      "55000/55000 [==============================] - 2s 32us/step - loss: 0.0531 - acc: 0.7633\n",
      "Epoch 159/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0529 - acc: 0.7641\n",
      "Epoch 160/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55000/55000 [==============================] - 1s 27us/step - loss: 0.0528 - acc: 0.7652\n",
      "Epoch 161/200\n",
      "55000/55000 [==============================] - 1s 26us/step - loss: 0.0526 - acc: 0.7664\n",
      "Epoch 162/200\n",
      "55000/55000 [==============================] - 1s 26us/step - loss: 0.0524 - acc: 0.7671\n",
      "Epoch 163/200\n",
      "55000/55000 [==============================] - 1s 26us/step - loss: 0.0522 - acc: 0.7683\n",
      "Epoch 164/200\n",
      "55000/55000 [==============================] - 1s 26us/step - loss: 0.0520 - acc: 0.7691\n",
      "Epoch 165/200\n",
      "55000/55000 [==============================] - 1s 26us/step - loss: 0.0519 - acc: 0.7699\n",
      "Epoch 166/200\n",
      "55000/55000 [==============================] - 2s 27us/step - loss: 0.0517 - acc: 0.7707\n",
      "Epoch 167/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0515 - acc: 0.7715\n",
      "Epoch 168/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0513 - acc: 0.7727\n",
      "Epoch 169/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0512 - acc: 0.7735\n",
      "Epoch 170/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0510 - acc: 0.7746\n",
      "Epoch 171/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0508 - acc: 0.7753\n",
      "Epoch 172/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0507 - acc: 0.7762\n",
      "Epoch 173/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0505 - acc: 0.7770\n",
      "Epoch 174/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0503 - acc: 0.7778\n",
      "Epoch 175/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0502 - acc: 0.7787\n",
      "Epoch 176/200\n",
      "55000/55000 [==============================] - 2s 28us/step - loss: 0.0500 - acc: 0.7797\n",
      "Epoch 177/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0498 - acc: 0.7805\n",
      "Epoch 178/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0497 - acc: 0.7815\n",
      "Epoch 179/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0495 - acc: 0.7820\n",
      "Epoch 180/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0494 - acc: 0.7830\n",
      "Epoch 181/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0492 - acc: 0.7837\n",
      "Epoch 182/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0491 - acc: 0.7845\n",
      "Epoch 183/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0489 - acc: 0.7853\n",
      "Epoch 184/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0487 - acc: 0.7861\n",
      "Epoch 185/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0486 - acc: 0.7867\n",
      "Epoch 186/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0484 - acc: 0.7873\n",
      "Epoch 187/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0483 - acc: 0.7881\n",
      "Epoch 188/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0482 - acc: 0.7887\n",
      "Epoch 189/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0480 - acc: 0.7893\n",
      "Epoch 190/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0479 - acc: 0.7903\n",
      "Epoch 191/200\n",
      "55000/55000 [==============================] - ETA: 0s - loss: 0.0477 - acc: 0.791 - 2s 30us/step - loss: 0.0477 - acc: 0.7911\n",
      "Epoch 192/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0476 - acc: 0.7918\n",
      "Epoch 193/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0474 - acc: 0.7925\n",
      "Epoch 194/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0473 - acc: 0.7933\n",
      "Epoch 195/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0471 - acc: 0.7940\n",
      "Epoch 196/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0470 - acc: 0.7949\n",
      "Epoch 197/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0469 - acc: 0.7955\n",
      "Epoch 198/200\n",
      "55000/55000 [==============================] - 2s 29us/step - loss: 0.0467 - acc: 0.7962\n",
      "Epoch 199/200\n",
      "55000/55000 [==============================] - 2s 30us/step - loss: 0.0466 - acc: 0.7968\n",
      "Epoch 200/200\n",
      "55000/55000 [==============================] - 2s 31us/step - loss: 0.0465 - acc: 0.7975\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f2225b847f0>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "\n",
    "model = Sequential() #we begin stack layers like pancakes\n",
    "model.add(Dense(32, input_dim=784, activation=\"sigmoid\")) #input-hidden connection\n",
    "model.add(Dense(10, activation=\"sigmoid\")) #hidden-output connection\n",
    "\n",
    "model.compile(loss='mean_squared_error', optimizer='sgd', metrics=['accuracy']) #we define loss + algorithm of descent\n",
    "model.fit(mnist.train.images,mnist.train.labels,epochs=200,batch_size=128) \n",
    "#epoch - number of iteratons over whole data\n",
    "#neural net takes batch_size of examples, calculates average gardient and applies it\n",
    "#in class NeuralNet batch_size is always 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAATUAAAE/CAYAAAAnhFRiAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4xLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvAOZPmwAAE5RJREFUeJzt3X2wVPV9x/HPJ4h0EJ1guRgfqMSH\nkBBmQvQG60MdY0x8SCzRaRKIGBxscRrIxEmmE8c4Uds6oSY+TWPNoFJNo7EOPk51YpRJQkxb5YKE\nBynFIFGQwLWIYpoQgW//2ENmoffePXd37927X96vmZ27e/Z7fud7OPDhnN1zznVECACyeFerGwCA\nZiLUAKRCqAFIhVADkAqhBiAVQg1AKoRacravs/39Pt5fbfusfo75Z7bXNtzcALJ9j+2/b3UfGHyE\nWpuz/XbVY4/t31a9vqTW/BHxwYj4SX+WGRE/i4gJdTedgO0bbb9q+y3bv7L99Vb3hApCrc1FxKi9\nD0mvSLqwatp9re4vA9vDeph8t6T3R8Rhkk6T9HnbFw9uZ+gJoXZgONj292zvKA43O/e+YXuD7XOK\n51NsdxV7H1ts39zTYLbPsr2x6vXXbG8qxl9r+2O9zHeP7dttP1HUPmf7+OK98bbD9kFV9T+x/ZfF\n88ts/9z2Lba3215v+7Ri+qu2t9qeud8ix9h+uljWT20fWzX2+4v3thU9f3a/Pu+w/aTt30j66P7r\nEhFrI+I3VZP2SDqhp/XG4CLUDgx/LukBSe+W9Lik7/RSd5uk24q9j+MlPVhrYNsTJM2V9JGIOFTS\nuZI29DHLdEnXSxot6SVJN5RbBUnSKZJWSPpjSfersk4fUSVMZkj6ju1RVfWXSPo7SWMkLZd0X9Hz\nIZKeLsYYW/T0T7Y/WDXv54veDpX0bE/N2L7K9tuSNko6pBgPLUaoHRiejYgnI2K3pH+R9KFe6t6R\ndILtMRHxdkT8Z4mxd0saIWmi7eERsSEiftlH/cMR8XxE7FIlZCb3Yz1ejoh/LtbjXyWNk/S3EbEz\nIn4k6ffad2/piYhYHBE7JX1d0qm2x0n6lKQNxVi7ImKZpIck/UXVvI9FxM8jYk9E/K6nZiJiniqh\nd5Iqf65v9mNdMEAItQPDr6ue/6+kP6o+zKtyuaT3Sfov20tsf6rWwBHxkqQrJV0naavtB2wf1Y9e\nRvVW2IMtVc9/Wyx//2nV471a1efbkrZJOkrSsZJOKQ5jt9verspe3Xt6mrcvUfFCsezr+7EuGCCE\nGv4gItZFxHRVDsn+QdLC4lCt1nz3R8QZqoRFFPP2197Pp0ZWTXtPT4X9MG7vk+Kw9HBJr6kSWD+N\niHdXPUZFxF9Xzdvf29ccpMohO1qMUMMf2J5huyMi9kjaXkzeXWOeCbbPtj1C0u9U2WPpc56eRES3\npE2SZtgeZnuWGg+JC2yfYftgVT5bey4iXpX0b5LeZ/tS28OLx0dsf6DMoLbfZfsK26NdMUXSHEmL\nGuwXTUCoodp5klYXH37fJmlab58nVRkhaZ6k11U5tBwr6eo6l/9Xkv5G0v9I+qCkf69znL3ul3St\nKoedJ6tyiKmI2CHpE5KmqbLn9mtV9i5H9GPsiyT9UtIOSd+X9I/FAy1mbhIJIBP21ACkQqgBSIVQ\nA5AKoQYgFUINQCo9nVU+YMaMGRPjx48fzEUCSGLp0qWvR0RHrbqGQs32eaqczzRM0l3FtXC9Gj9+\nvLq6uhpZJIADlO1flamr+/CzuMfU7ZLOlzRR0nTbE+sdDwCaoZHP1KZIeiki1kfE71W5DczU5rQF\nAPVpJNSO1r53MthYTAOAlmkk1NzDtP93zZXt2cXdVLu6u7sbWBwA1NZIqG1U1a1dJB2jysXB+4iI\n+RHRGRGdHR01v7gAgIY0EmpLJJ1o+73FrV2mqXKraABombpP6YiIXbbnSnpKlVM6FkTE6qZ1BgB1\naOg8tYh4UtKTTeoFABrGZVIAUiHUAKRCqAFIhVADkAqhBiAVQg1AKoQagFQINQCpEGoAUiHUAKRC\nqAFIhVADkAqhBiAVQg1AKoQagFQINQCpEGoAUiHUAKRCqAFIhVADkAqhBiAVQg1AKoQagFQINQCp\nEGoAUiHUAKRCqAFIhVADkAqhBiAVQg1AKoQagFQINQCpEGoAUiHUAKRCqAFIhVADkAqhBiAVQg1A\nKoQagFQOanUDaG8vvvhizZpbb7211Fh33nlno+302znnnFOq7oknnihVd/DBBzfSDpqgoVCzvUHS\nDkm7Je2KiM5mNAUA9WrGntpHI+L1JowDAA3jMzUAqTQaaiHpR7aX2p7dU4Ht2ba7bHd1d3c3uDgA\n6FujoXZ6RJwk6XxJc2yfuX9BRMyPiM6I6Ozo6GhwcQDQt4ZCLSJeK35ulfSIpCnNaAoA6lV3qNk+\nxPahe59L+oSkVc1qDADq0ci3n0dIesT23nHuj4gfNqUrAKhT3aEWEeslfaiJvWAQLFu2rFTd7bff\nXqpu4cKFNWt27NhRaqziP8hBtWjRolJ1O3fuLFXHybetxykdAFIh1ACkQqgBSIVQA5AKoQYgFUIN\nQCqEGoBUCDUAqRBqAFLhdt5JLFmypFTdJz/5yVJ1r7/OfT+r/fCH5a4ALPPnO3LkyEbbQR/YUwOQ\nCqEGIBVCDUAqhBqAVAg1AKkQagBSIdQApEKoAUiFUAOQClcUJLFgwYJSdVwpUJ/Pfe5zpeqmTKn9\nWyKvv/76UmOde+65peqwL/bUAKRCqAFIhVADkAqhBiAVQg1AKoQagFQINQCpEGoAUuHk2yS+8IUv\nlKp75plnBriT9rJt27ZSdW+88Uapuueff75mzbx580qNxcm39WFPDUAqhBqAVAg1AKkQagBSIdQA\npEKoAUiFUAOQCqEGIBVCDUAqXFGQxKmnnlqqbt26dQPcydAQEaXqvvGNb5Squ+GGGxppB4Oo5p6a\n7QW2t9peVTXtcNtP215X/Bw9sG0CQDllDj/vkXTeftOukrQoIk6UtKh4DQAtVzPUImKxpP2v+p0q\n6d7i+b2SPt3kvgCgLvV+UXBERGyWpOLn2Oa1BAD1G/BvP23Ptt1lu6u7u3ugFwfgAFdvqG2xfaQk\nFT+39lYYEfMjojMiOjs6OupcHACUU2+oPS5pZvF8pqTHmtMOADSmzCkdP5D0H5Im2N5o+3JJ8yR9\n3PY6SR8vXgNAy9U8+TYipvfy1sea3AsANIwrCtB2HnzwwZo1y5cvLzVW2d8XUNakSZNq1syaNaup\ny8S+uPYTQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5AKoQYgFUINQCpcUYAh45FHHilV961vfatm\nzdKlSxttpy6XX355zZpLL710EDo5cLGnBiAVQg1AKoQagFQINQCpEGoAUiHUAKRCqAFIhVADkAon\n36JHDzzwQKm6PXv21Ky57bbbSo21YsWKUnU7d+4sVddMN910U6m6OXPmDHAnqIU9NQCpEGoAUiHU\nAKRCqAFIhVADkAqhBiAVQg1AKoQagFQINQCpcEVBEi+88EKpulmzZpWqK3t2f0SUqmt3a9euLVX3\nzjvv1Kw56CD+2Q0k9tQApEKoAUiFUAOQCqEGIBVCDUAqhBqAVAg1AKkQagBSIdQApMKpzW2gq6ur\nZs3FF19caqyNGzc22s6AGTlyZKm6ESNG1Kx54403Gm1nH/Pnzy9VN2PGjJo1Z5xxRqPtoA8199Rs\nL7C91faqqmnX2d5ke3nxuGBg2wSAcsocft4j6bwept8SEZOLx5PNbQsA6lMz1CJisaRtg9ALADSs\nkS8K5tpeURyeju6tyPZs2122u7q7uxtYHADUVm+o3SHpeEmTJW2W1Otveo2I+RHRGRGdHR0ddS4O\nAMqpK9QiYktE7I6IPZLulDSluW0BQH3qCjXbR1a9vEjSqt5qAWAw1TxPzfYPJJ0laYztjZKulXSW\n7cmSQtIGSVcMYI8AUFrNUIuI6T1MvnsAekEvNm3aVLNm+/btTV3mtGnTStVdeOGFTVvm6NG9ft+0\nj8MOO6xmzfr160uN9aUvfalU3ZtvvlmqrszJt88880ypsU444YRSddgXl0kBSIVQA5AKoQYgFUIN\nQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIXbebeBqVOn1qx59NFHS421evXqUnVf/OIXS9UNGzasVN1g\nO+2000rVvfzyy6Xqrr322lJ1r7zySs2abdu4PeFAYk8NQCqEGoBUCDUAqRBqAFIh1ACkQqgBSIVQ\nA5AKoQYgFUINQCpcUZDE2Wef3dS6drdr165SdYsXLx7gTjDY2FMDkAqhBiAVQg1AKoQagFQINQCp\nEGoAUiHUAKRCqAFIhVADkApXFKDtbNq0qWbNxIkTS421Y8eORtvZxze/+c2aNSeffHJTl4l9sacG\nIBVCDUAqhBqAVAg1AKkQagBSIdQApEKoAUiFUAOQCiffYsBt2bKlVN1ll11Wqm7NmjU1a5p9Uu0l\nl1xSqu4rX/lKzZphw4Y12g76UHNPzfY42z+2vcb2attfLqYfbvtp2+uKn6MHvl0A6FuZw89dkr4a\nER+Q9KeS5tieKOkqSYsi4kRJi4rXANBSNUMtIjZHxLLi+Q5JayQdLWmqpHuLsnslfXqgmgSAsvr1\nRYHt8ZI+LOk5SUdExGapEnySxja7OQDor9KhZnuUpIckXRkRb/Vjvtm2u2x3dXd319MjAJRWKtRs\nD1cl0O6LiIeLyVtsH1m8f6SkrT3NGxHzI6IzIjo7Ojqa0TMA9KrMt5+WdLekNRFxc9Vbj0uaWTyf\nKemx5rcHAP1T5jy10yVdKmml7eXFtKslzZP0oO3LJb0i6TMD0yIAlFcz1CLiWUnu5e2PNbcdAGgM\nVxSgR2VumS1JCxcurFnz3e9+t9RYa9euLVXXTMcee2ypumuuuaZU3fDhwxtpB03AtZ8AUiHUAKRC\nqAFIhVADkAqhBiAVQg1AKoQagFQINQCpEGoAUuGKgha66qpyNws+6aSTatZMmjSp1FhPPfVUqboF\nCxaUqlu9enWpumY65phjatbMmDGj1FgzZ86sXSRpwoQJperQeuypAUiFUAOQCqEGIBVCDUAqhBqA\nVAg1AKkQagBSIdQApMLJty20YsWKUnU33njjAHcysE455ZRSdVdccUWpujPPPLNmzXHHHVdqLOTD\nnhqAVAg1AKkQagBSIdQApEKoAUiFUAOQCqEGIBVCDUAqhBqAVLiioIXuuuuuUnXnn39+zZqVK1c2\n2k5drrnmmpo1c+fOLTXW2LFjG20HYE8NQC6EGoBUCDUAqRBqAFIh1ACkQqgBSIVQA5AKoQYgFUIN\nQCpcUdBCRx11VKm6X/ziFwPcCZBHzT012+Ns/9j2GturbX+5mH6d7U22lxePCwa+XQDoW5k9tV2S\nvhoRy2wfKmmp7aeL926JiG8PXHsA0D81Qy0iNkvaXDzfYXuNpKMHujEAqEe/viiwPV7ShyU9V0ya\na3uF7QW2Rze5NwDot9KhZnuUpIckXRkRb0m6Q9Lxkiarsid3Uy/zzbbdZburu7u7CS0DQO9KhZrt\n4aoE2n0R8bAkRcSWiNgdEXsk3SlpSk/zRsT8iOiMiM6Ojo5m9Q0APSrz7acl3S1pTUTcXDX9yKqy\niyStan57ANA/Zb79PF3SpZJW2l5eTLta0nTbkyWFpA2SrhiQDgGgH8p8+/msJPfw1pPNbwcAGsNl\nUgBSIdQApEKoAUiFUAOQCqEGIBVCDUAqhBqAVAg1AKkQagBSIdQApEKoAUiFUAOQCqEGIBVCDUAq\nhBqAVAg1AKkQagBSIdQApEKoAUjFETF4C7O7Jf1qv8ljJL0+aE00X7v3L7X/OrR7/1L7r8Ng9H9s\nRNT8PZuDGmo9NmB3RURnS5toQLv3L7X/OrR7/1L7r8NQ6p/DTwCpEGoAUhkKoTa/1Q00qN37l9p/\nHdq9f6n912HI9N/yz9QAoJmGwp4aADRNy0LN9nm219p+yfZVreqjEbY32F5pe7ntrlb3U4btBba3\n2l5VNe1w20/bXlf8HN3KHvvSS//X2d5UbIflti9oZY99sT3O9o9tr7G92vaXi+nttA16W4chsR1a\ncvhpe5ik/5b0cUkbJS2RND0iXhz0Zhpge4Okzohom/OLbJ8p6W1J34uIScW0GyVti4h5xX8woyPi\na63ssze99H+dpLcj4tut7K0M20dKOjIiltk+VNJSSZ+WdJnaZxv0tg6f1RDYDq3aU5si6aWIWB8R\nv5f0gKSpLerlgBIRiyVt22/yVEn3Fs/vVeUv6JDUS/9tIyI2R8Sy4vkOSWskHa322ga9rcOQ0KpQ\nO1rSq1WvN2oI/aH0Q0j6ke2ltme3upkGHBERm6XKX1hJY1vcTz3m2l5RHJ4O2UO3arbHS/qwpOfU\npttgv3WQhsB2aFWouYdp7fg17OkRcZKk8yXNKQ6NMPjukHS8pMmSNku6qbXt1GZ7lKSHJF0ZEW+1\nup969LAOQ2I7tCrUNkoaV/X6GEmvtaiXukXEa8XPrZIeUeWwuh1tKT4n2ft5ydYW99MvEbElInZH\nxB5Jd2qIbwfbw1UJg/si4uFiclttg57WYahsh1aF2hJJJ9p+r+2DJU2T9HiLeqmL7UOKD0ll+xBJ\nn5C0qu+5hqzHJc0sns+U9FgLe+m3vWFQuEhDeDvYtqS7Ja2JiJur3mqbbdDbOgyV7dCyk2+Lr3tv\nlTRM0oKIuKEljdTJ9nGq7J1J0kGS7m+HdbD9A0lnqXJXhS2SrpX0qKQHJf2JpFckfSYihuSH8b30\nf5YqhzwhaYOkK/Z+PjXU2D5D0s8krZS0p5h8tSqfSbXLNuhtHaZrCGwHrigAkApXFABIhVADkAqh\nBiAVQg1AKoQagFQINQCpEGoAUiHUAKTyf5QRQbZaKl7QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f22214ed898>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "i=random.randint(0,54999)\n",
    "fig=plt.figure(figsize=(5,5))\n",
    "plt.imshow(mnist.train.images[i].reshape((28,28)),cmap=plt.get_cmap('binary'))\n",
    "digit=np.argmax(model.predict(np.array([mnist.train.images[i]]))) #model.predict returns array of y predictions based on x values\n",
    "plt.title('This is number {0}'.format(digit))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see that Keras has very simple syntax and if you learn most often used classes, you can very easily experiment with data and create individual projects. Finally, Keras models can be saved in HDF5 files as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('keras_model.h5')  # creates a HDF5 file 'my_model.h5'\n",
    "del model  # deletes the existing model\n",
    "\n",
    "# returns a compiled model\n",
    "# identical to the previous one\n",
    "model = load_model('keras_model.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://keras.io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Object mnist contains also mnist.test.images and mnist.test.labels, which are 10000 x-y pairs. Train neural nets with different numbers of hidden neurons. How changes loss on train data and on test data? Try doing this task in both bold Python and Keras.\n",
    "2. Find or gather yourself enough labeled data (which consists from x-y pairs), build neural network and train it using tips you have learned during this lecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
