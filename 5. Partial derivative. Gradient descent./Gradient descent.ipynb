{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial derivative. Gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the simplest cases, search of local minimums can be done by solving system of equations from the first part of the lesson. However, with increase in numbers of parametres or in complexity of functions, search for a solution can be time consuming or even analytically impossible. That is why, we are going to use iterative algorithm called **gradient descent**, which will approximately converge to a local minimum (or maximum). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider the descent of people on a mountain. Since, mountain is huge and it is darkening, the group can see only several metres around. They walk with constant speed, and their goal is the *fastest* descent possible. If only they had a map! Than it would be an easy optimization problem. \n",
    "![Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/f/ff/Gradient_descent.svg/512px-Gradient_descent.svg.png)\n",
    "However, they still have got a hope. It turns out, that they might reach local minimum if they are moving in opposite direction to direction of the *gradient*. We won't get deep into definition of the gradient because it is quite complex mathematical concept. However, we are going to take without proof one property of the gradient in Cartesian coordinates and reformulate gradient descent rule on a language of partial derivatives:\n",
    "> To find local minimum of a function $f$, you should *substract* from each coordinate value proportional to derivative of given function $f$ with respect to current coordinate.\n",
    "In mathematical notations: $$x'_i=x_i-\\eta\\frac{\\partial f}{\\partial x_i},\\text{ for each i from 1 to n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider function with two variables $$f(x,y)=x^2+y^2$$ We will try to find *global* minimum of the function using gradient descent. First of all, we are going to define the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return x**2+y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have 2 variables, we are going to derive two partial derivatives $\\frac{\\partial f}{\\partial x}$ and $\\frac{\\partial f}{\\partial y}$ analytically and then implement them as a Python function with 2 inputs (x and y) and 2 outputs (2 partial derivatives):\n",
    "$$\\begin{align}\\frac{\\partial f}{\\partial x}=2x \\\\ \\frac{\\partial f}{\\partial y}=2y\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pd(x,y):\n",
    "    return 2*x,2*y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thirdly, we are going to build color graph of a function for better understanding (blue represents smaller values and yellow larger one):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "a=1\n",
    "na=200\n",
    "xx=np.linspace(-a,a,na)\n",
    "yy=np.linspace(-a,a,na).reshape(-1, 1)\n",
    "fig=plt.figure(figsize=(7,7))\n",
    "ax=plt.subplot(111)\n",
    "im = ax.imshow(f(xx,yy),cmap='viridis',extent=[-a,a,-a,a],origin='lower')\n",
    "fig.colorbar(im, shrink=0.5, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's place a starting point at coordinates (0.8;0.6) and draw path of the point depending on a value of *step size* $\\eta$, where the red dot is the initial position and the yellow dot is the final position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "eta_list=[0.001,0.01,0.1,0.5,1.01] #lists of step sizes \n",
    "\n",
    "for eta in eta_list:\n",
    "    x=0.8  #initial coordinates\n",
    "    y=0.6\n",
    "    x_list=[] #lists of coordinates\n",
    "    y_list=[]\n",
    "    for i in range(50):\n",
    "        x_list.append(x) #writing down coordinates\n",
    "        y_list.append(y)\n",
    "        dx,dy=pd(x,y) #calculating partial derivatives\n",
    "        x-=eta*dx #gradient descent!\n",
    "        y-=eta*dy\n",
    "    \n",
    "    #creating graph\n",
    "    x_list=np.array(x_list)\n",
    "    y_list=np.array(y_list)\n",
    "    a=1\n",
    "    na=200\n",
    "    xx=np.linspace(-a,a,na)\n",
    "    yy=np.linspace(-a,a,na).reshape(-1, 1)\n",
    "    fig=plt.figure(figsize=(5,5))\n",
    "    ax=plt.subplot(111)\n",
    "    im = ax.imshow(f(xx,yy),cmap='viridis',extent=[-a,a,-a,a],origin='lower')\n",
    "    ax.plot(x_list,y_list,'r-')\n",
    "    ax.plot([x_list[0]],[y_list[0]],'ro',markersize=7) #adding starting point as a red circle\n",
    "    ax.plot([x_list[len(x_list)-1]],[y_list[len(x_list)-1]],'yo',markersize=7) #adding final point as an yellow circle\n",
    "    plt.title('$\\eta='+str(eta)+'$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From graphs, we can observe that small step size is computationally ineffective because it requires a lot of steps to reach its goal. On the other hand, large step size can make point *oscillate* around minimum never reaching it. Thus, step size is important *hyperparameter* of gradient descent. If we have chosen it right ($\\eta$ is between 0.1 and 0.5) then it reaches its goal of finding the global minimum in 50 steps. However, it have done greatly because there is, in fact, only one minimum. What will happen if we add some local but not global minimums?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counter example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider function with two variables $$f(x,y)=x^2+\\sin(12\\pi xy)+y^2$$ We are going to go through the same steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def f(x,y):\n",
    "    return x**2+np.sin(12*np.pi*x*y)+y**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{align}\\frac{\\partial f}{\\partial x}=2x+12\\pi y\\cos(12\\pi xy) \\\\ \\frac{\\partial f}{\\partial y}=2y+12\\pi x\\cos(12\\pi xy)\\end{align}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pd(x,y):\n",
    "    return 2*x+np.cos(12*np.pi*x*y)*(12*np.pi*y),np.cos(12*np.pi*x*y)*(12*np.pi*x)+2*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "a=1\n",
    "na=200\n",
    "xx=np.linspace(-a,a,na)\n",
    "yy=np.linspace(-a,a,na).reshape(-1, 1)\n",
    "fig=plt.figure(figsize=(7,7))\n",
    "ax=plt.subplot(111)\n",
    "im = ax.imshow(f(xx,yy),cmap='viridis',extent=[-a,a,-a,a],origin='lower')\n",
    "fig.colorbar(im, shrink=0.5, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "eta_list=[0.001,0.002,0.005,0.01,0.02,0.03,0.04] #lists of step sizes \n",
    "\n",
    "for eta in eta_list:\n",
    "    x=0.8  #initial coordinates\n",
    "    y=0.6\n",
    "    x_list=[] #lists of coordinates\n",
    "    y_list=[]\n",
    "    for i in range(50):\n",
    "        x_list.append(x) #writing down coordinates\n",
    "        y_list.append(y)\n",
    "        dx,dy=pd(x,y) #calculating partial derivatives\n",
    "        x-=eta*dx #gradient descent!\n",
    "        y-=eta*dy\n",
    "    \n",
    "    #creating graph\n",
    "    x_list=np.array(x_list)\n",
    "    y_list=np.array(y_list)\n",
    "    a=1\n",
    "    na=200\n",
    "    xx=np.linspace(-a,a,na)\n",
    "    yy=np.linspace(-a,a,na).reshape(-1, 1)\n",
    "    fig=plt.figure(figsize=(5,5))\n",
    "    ax=plt.subplot(111)\n",
    "    im = ax.imshow(f(xx,yy),cmap='viridis',extent=[-a,a,-a,a],origin='lower')\n",
    "    ax.plot(x_list,y_list,'r-')\n",
    "    ax.plot([x_list[0]],[y_list[0]],'ro',markersize=7) #adding starting point as a red circle\n",
    "    ax.plot([x_list[len(x_list)-1]],[y_list[len(x_list)-1]],'yo',markersize=7) #adding final point as an yellow circle\n",
    "    plt.title('$\\eta='+str(eta)+'$')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With adding more minimums and breaking overall central symmetry of a function we could observe appearance of new types of behaviours:\n",
    "1. Zig-zagging <br>\n",
    "Once point reached region of local minimum which is too deep (point doesn't have enough \"velocity\" to overcome barrier) it begins zig-zagging around local minimum. This effect is especially strong for quite high values of step size and functions with shallow minimum.\n",
    "2. Local minimums\n",
    "Gradient descent doesn't know all information about function - it knows only about local region. That is why it is natural that gradient descent can't find global minimums in most cases. This problem can be partly solved by increasing step size but you should be careful because point might began oscillating ($\\eta\\gtrsim 0.04$ for current example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write function grad_descent(f,df,hyperp), where f - a function we are optimizing on, df - function which returns all partial derivatives of the function with respect to each variable, hyperp=[x,eta], where x - list of initial coordinates and eta - step size. Add it to opt_lib.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximate data with a line using gradient descent. Build a graph of the data and approximation line you got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "x=[1,2,3,4,5] #x data\n",
    "y=[2,3,6,8,9] #y data\n",
    "a=2*random.random()-1 #initial points\n",
    "b=2*random.random()-1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Does your line is globally optimal? For answering this question, I am including color plot of error depending on line parametres $a$ and $b$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def E(x,y,a,b):\n",
    "    e=0\n",
    "    for i in range(len(x)):\n",
    "        e+=(y[i]-a*x[i]-b)**2\n",
    "        \n",
    "    return e\n",
    "\n",
    "x=[1,2,3,4,5] #x data\n",
    "y=[2,3,6,8,9] #y data\n",
    "\n",
    "s=3\n",
    "ns=200\n",
    "aa=np.linspace(-s+2,s+2,ns)\n",
    "bb=np.linspace(-s,s,ns).reshape(-1, 1)\n",
    "fig=plt.figure(figsize=(7,7))\n",
    "ax=plt.subplot(111)\n",
    "im = ax.imshow(E(x,y,aa,bb),cmap='viridis',extent=[-s+2,s+2,-s,s],origin='lower')\n",
    "fig.colorbar(im, shrink=0.5, aspect=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://ruder.io/optimizing-gradient-descent/ - during this lesson we have considered so-called *vanilla* gradient descent. This excellent article overviews different improvements in gradient descent. You can read it now or after several lessons when we are going to return to gradient descent for much greater practical use."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
