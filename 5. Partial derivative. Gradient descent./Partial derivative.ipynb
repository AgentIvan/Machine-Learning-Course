{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial derivative. Gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partial derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider three dimensional graph of function with two varibales: $f(x,y)=x^2+xy+y^2$\n",
    "![Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/2/2d/Partial_func_eg.svg/800px-Partial_func_eg.svg.png)\n",
    "We can take slice of this graph at Oxz plane at **constant** value of y=1\n",
    "![Wikipedia](https://upload.wikimedia.org/wikipedia/commons/thumb/f/fe/X2%2BX%2B1.svg/300px-X2%2BX%2B1.svg.png)\n",
    "And now we can denote graph's slope, for instance, at point x=1 as **partial derivative** of function at points $x=1;y=1$. And that's it! <br>\n",
    "To understand how to do it analytically with knowledge of taking derivative of function with one variable, let's recollect how we have build slice of 3D example graph. We took slice at **constant** y value and found slope of slice. From geometrical meaning of derivative, it can be implied that we have found derivative of function $g(x)=f(x,1)$ - we have reduced number of variables in example function. Thus, we can find derivative of function $g(x)$ analytically or just find derivative of function $f(x,y)$ regarding to x and treating y as a **constant**. <br>\n",
    "Whole this process can be written in mathematical symbols as $$\\frac{\\partial f(x,y)}{\\partial x}=\\frac{df_y(x)}{dx}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More than two variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we have more than two variables, it is hard to visualize the graph, as it was discussed during Lesson 2. However, we can still take 2D slices of multidimensional graphs analytically. More strictly, if we have function with n variables $f(x_1,x_2,...,x_n)$ partial derivative with respect to the variable $x_i$ is equal to $$\\frac{\\partial f(x_1,x_2,...,x_n)}{\\partial x_i}=\\frac{df_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)}{dx_i}$$\n",
    "where denotion $f_{x_1,...,x_{i-1},x_{i+1},...x_n}(x_i)$ represents fact that values $x_1,...,x_{i-1},x_{i+1},...x_n$ are fixed values and are treated as constants."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's consider function with 3 variables $$f(x,y,z)=\\frac{e^{xyz}}{x+y+z}$$ which is going to showcase practically all rules of differentiation with consideration of partial derivatives (you can use all rules but keep in mind that other variables are constant): $$\\frac{\\partial f}{\\partial x}=\\frac{\\frac{\\partial}{\\partial x}(e^{xyz})\\cdot(x+y+z)-e^{xyz}\\cdot\\frac{\\partial}{\\partial x}(x+y+z)}{(x+y+z)^2}=\\frac{e^{xyz}\\cdot\\frac{\\partial}{\\partial x}(xyz)\\cdot(x+y+z)-e^{xyz}\\cdot 1}{(x+y+z)^2}=\\frac{e^{xyz}}{(x+y+z)^2}(yz(x+y+z)-1)$$ <br>\n",
    "The same can be done for y and z variables, but the result will be symmetrical and, therefore, not very interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fermat's theorem "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you built graphs of different functions and explored their regions where derivative is equal to 0, as a part of Lesson 4 homework, you could notice 3 different behaviours near x values where derivative is equal to 0. Let's sum up these behaviours on matplotlib graph one more time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x=[]\n",
    "y1=[]\n",
    "y2=[]\n",
    "y3=[]\n",
    "\n",
    "for i in range(21): \n",
    "    x.append((i-10)*0.1)\n",
    "    y1.append(x[i]**2)\n",
    "    y2.append(-x[i]**2)\n",
    "    y3.append(x[i]**3)\n",
    "\n",
    "fig = plt.figure(figsize=(9.7,6))\n",
    "plt.subplot(131)\n",
    "plt.plot(x,y1)\n",
    "plt.title('minimum')\n",
    "\n",
    "plt.subplot(132)\n",
    "plt.plot(x,y2)\n",
    "plt.title('maximum')\n",
    "\n",
    "plt.subplot(133)\n",
    "plt.plot(x,y3)\n",
    "plt.title('saddle')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Two behaviours (minimum and maximum) are key for optimization problems while saddle behaviour is obviously not. Nonetheless, you can already guess what condition defines occurence of all 3 cases - derivative of a function is equal to 0. This fact is strictly proven in calculus and shaped in Fermat's theorem:\n",
    ">  *If a function has a local extremum at some point and is differentiable there, then the function's derivative at that point must be zero.*\n",
    "\n",
    "Local extremum is a point of either local minimum or maximum. Thus, we have obtained powerful rule which can find local minimums or maximums. However, we still cannot distinguish all 3 cases. This question will be discussed at second part of this lesson."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multivariable case"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a multivariable function has a local extremum at some points, 2D graphs-slices of this function around this point have local extremum too. This brings to\n",
    ">*If a multivariable function has a local extremum at some point and is differentiable there, then function's partial derivatives with respect to each variable must be zero.*\n",
    "\n",
    "This statement can be rewritten as a system of n equations with n variables (where n is number of function's variables): $$\\begin{cases}\\frac{\\partial f}{\\partial x_1}=0\\\\...\\\\\\frac{\\partial f}{\\partial x_n}=0\\end{cases}$$ <br>\n",
    "Since each slice of graph of a multivariable function can be a local minimum or a local maximum or a saddle point, number of different variants of function's behaviour begins to quickly grow in number. That is why mathematicians categorize all possibilities as:\n",
    "1. Local minimum - all slices have local minimum\n",
    "2. Local maximum - all slices have local maximum\n",
    "3. Saddle point - neither local minimum nor local maximum\n",
    "Still, I am going to illustrate all 6 possibilities for 3D graph of function $f(x,y)$ with two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "fig = plt.figure(figsize=(13,10))\n",
    "ax1=fig.add_subplot(2, 3, 1, projection='3d')\n",
    "ax1.set_title('minimum')\n",
    "ax2=fig.add_subplot(2, 3, 2, projection='3d')\n",
    "ax2.set_title('maximum')\n",
    "ax3=fig.add_subplot(2, 3, 3, projection='3d')\n",
    "ax3.set_title('saddle point')\n",
    "ax4=fig.add_subplot(2, 3, 4, projection='3d')\n",
    "ax4.set_title('saddle point')\n",
    "ax5=fig.add_subplot(2, 3, 5, projection='3d')\n",
    "ax5.set_title('saddle point')\n",
    "ax6=fig.add_subplot(2, 3, 6, projection='3d')\n",
    "ax6.set_title('saddle point')\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-1.5, 1.5, 0.25)\n",
    "Y = np.arange(-1.5, 1.5, 0.25)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "Z1=X**2+Y**2\n",
    "Z2=-X**2-Y**2\n",
    "Z3=X**2-Y**2\n",
    "Z4=X**3+Y**2\n",
    "Z5=X**3-Y**2\n",
    "Z6=X**3+Y**3\n",
    "\n",
    "# Plot the surface.\n",
    "surf1 = ax1.plot_surface(X, Y, Z1, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "surf2 = ax2.plot_surface(X, Y, Z2, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "surf3 = ax3.plot_surface(X, Y, Z3, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "surf4 = ax4.plot_surface(X, Y, Z4, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "surf5 = ax5.plot_surface(X, Y, Z5, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "surf6 = ax6.plot_surface(X, Y, Z6, cmap=cm.coolwarm, linewidth=0, antialiased=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While conducting research, in most cases, scientists need to make a correlation between a theory and an experimental data. They often use *linear approximation* of a data, which means that they try to find such line which corresponds to data the best. In the simplest case, our data consists of points with two coordinates. For example, you can measure temperature depending on a date. In that case, your x coordinate is date and y coordinate is temperature. <br>\n",
    "Take a look at example which shows that green line is better approximation than blue line:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x=[1,2,3,4,5]\n",
    "y=[2,3,6,8,9]\n",
    "green_x=[1,5]\n",
    "green_y=[1.5,9.5]\n",
    "blue_x=[1,5]\n",
    "blue_y=[2,15]\n",
    "\n",
    "plt.plot(x,y,'ro')\n",
    "plt.plot(green_x,green_y,'g-')\n",
    "plt.plot(blue_x,blue_y,'b-')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like an optimization problem! However, we still need to define two important issues:\n",
    "1. How to define line? What parametres to use? <br>\n",
    "The most common way to define line, especially not in math, is with two parametres $a$ and $b$, which completely define slope and intersection with Oy axis. In that case, line formula is $y=ax+b$.\n",
    "2. What are we going to optimize?\n",
    "Obviously, we have to define what is quality of approximation. Gauss and Legendre came up with idea of minimizing squares of offsets of the points from the line ![Wolfram MathWorld](http://mathworld.wolfram.com/images/eps-gif/LeastSquaresOffsets_1000.gif) <br>\n",
    "It seems as if perpendicular offsets are more natural to use. However, formulas become much simpler for vertical case. Moreover, vertical offsets can be used even for higher polynomials and, in fact, difference in results between these two methods is usually miniscule. Therefore, scientific community uses **least square method** with **vertical offsets**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can construct value which we will call **error** and denote as $E$ as a sum of squares of difference between experimental points' y-coordinate and line y-coordinate in the same x-coordinate. Coordinates of an i-th experimental point are denoted as $x_i$ and $y_i$ and number of all points as $N$. $$E=\\sum^N_{i=1}(y_i-(ax_i+b))^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, if we want optimize error $E$, which depends on parametres $a$ and $b$ (**experimental points' coordinates are not parametres - they are unchangeable**), from Fermat's theorem we need to solve system of 2 equation with 2 variables $a$ and $b$: $$\\begin{cases}\\frac{\\partial E}{\\partial a}=0 \\\\ \\frac{\\partial E}{\\partial b}=0\\end{cases}$$ $$\\begin{cases}\\sum^N_{i=1}2(y_i-ax_i-b)\\cdot(-x_i)=0 \\\\ \\sum^N_{i=1}2(y_i-ax_i-b)\\cdot(-1)=0 \\end{cases}$$ $$\\begin{cases}\\sum^N_{i=1}y_ix_i-a\\sum^N_{i=1}x_i^2-b\\sum^N_{i=1}x_i=0 \\\\ \\sum^N_{i=1}y_i-a\\sum^N_{i=1}x_i-bN=0 \\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got linear system of two equations with two variables but with a bit unusual coefficients. Its solution is: $$a=\\frac{N\\sum^N_{i=1}x_iy_i-\\sum^N_{i=1}x_i\\cdot\\sum^N_{i=1}y_i}{N\\sum^N_{i=1}x_i^2-\\Big(\\sum^N_{i=1}x_i\\Big)^2}$$ $$b=\\frac{1}{N}\\Big(\\sum^N_{i=1}y_i-a\\sum^N_{i=1}x_i\\Big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imagine a box with sides a,b,c. Given that total area of this box is S, find sides a,b,c of the box with maximum volume V and calculate it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write Python function least_squares(x,y) whose inputs are two lists with the same length and whose outputs are 2 numbers: a and b - parametres in line formula, which are calculated by final formulas above. Add this function to opt_lib.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/partial-derivative-and-gradient-articles/a/introduction-to-partial-derivatives"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
