{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feedforward neural network. Backpropagation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating output of neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now class \"NeuralNet\" is meaningless because we will use neural networks as approximators of sophisticated functions but this class can't even compute an output **y** based on an input **x**. Thus, based on weight notation, we are going to write code of computing next layer values based on previous ones and weights of connections between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1/(1+math.exp(-x))\n",
    "    \n",
    "weights=[]\n",
    "n_input=3\n",
    "n_hidden=4\n",
    "epsilon=0.1\n",
    "for i in range(n_input):\n",
    "    weights_row=[] #creating empty row of weights from one input neuron\n",
    "    for j in range(n_hidden):\n",
    "        weights_row.append(random.uniform(-epsilon,epsilon)) #appending random weights\n",
    "    weights.append(weights_row) #appending row to a table of weights    \n",
    "\n",
    "x=[]\n",
    "for i in range(n_input):\n",
    "    x.append(random.random())\n",
    "\n",
    "y=[]\n",
    "for j in range(n_hidden):\n",
    "    #preparing to sum product of values and weights\n",
    "    yj=0\n",
    "    for i in range(n_input):\n",
    "        yj+=x[i]*weights[i][j] #i - number of input neuron , j - number of output neuron\n",
    "        \n",
    "    yj=sigmoid(yj) #applying sigmoid function\n",
    "    y.append(yj)\n",
    "    \n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now this algorithm can be repeated twice in \"NeuralNet\" class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,inp,hid,out,epsilon):\n",
    "        #initial parameters are numbers of neurons in input (inp), hidden (hid), and output (out) layers \n",
    "        #plus epsilon value for weight initialization\n",
    "        #firstly, write down numbers of neurons in each layer \n",
    "        self.inp=inp\n",
    "        self.hid=hid\n",
    "        self.out=out\n",
    "        #secondly, create weights\n",
    "        #wa - weights of connection between input and hidden layers\n",
    "        #wb - weights of connection between hidden and output layers\n",
    "        import random\n",
    "        self.wa=[]\n",
    "        self.wb=[]\n",
    "        \n",
    "        #fill weight tables with random values from -epsilon to epsilon\n",
    "        for i in range(self.inp):\n",
    "            weights_row=[] \n",
    "            for j in range(self.hid):\n",
    "                weights_row.append(random.uniform(-epsilon,epsilon)) \n",
    "            self.wa.append(weights_row)  \n",
    "            \n",
    "        for i in range(self.hid):\n",
    "            weights_row=[] \n",
    "            for j in range(self.out):\n",
    "                weights_row.append(random.uniform(-epsilon,epsilon)) \n",
    "            self.wb.append(weights_row) \n",
    "            \n",
    "    #define activation function\n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+math.exp(-x))\n",
    "    \n",
    "    #calculate y output\n",
    "    def y(self,x):\n",
    "        \n",
    "        #firstly, calculate outputs of hidden layer neurons\n",
    "        hidden_values=[]\n",
    "        for j in range(self.hid):    \n",
    "            yj=0\n",
    "            for i in range(self.inp):\n",
    "                yj+=x[i]*self.wa[i][j] #i - number of input layer neuron , j - number of hidden layer neuron\n",
    "\n",
    "            yj=self.sigmoid(yj)\n",
    "            hidden_values.append(yj)\n",
    "\n",
    "        #secondly, hidden layer is treated as an input layer\n",
    "        output_values=[]\n",
    "        for k in range(self.out):\n",
    "            zk=0\n",
    "            for j in range(self.hid):\n",
    "                zk+=hidden_values[j]*self.wb[j][k] #j - number of hidden layer neuron , k - number of output layer neuron\n",
    "\n",
    "            zk=self.sigmoid(zk)\n",
    "            output_values.append(zk)\n",
    "            \n",
    "        return output_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn=NeuralNet(3,4,2,0.1) #implement neural net from a picture\n",
    "print(nn.y([1,0,1])) #get y for x=[1,0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main purpose of a neural network is still not accomplished. We need to figure out a way of changing something in a neural network to approximate functions. We have already declared its architecture and activation function, so we are left only with one thing to change - weights.\n",
    "Firstly, we need to show the way to a neural network by writing down not only x-values but also desired y-values. These pairs are called **training examples**. Such declaration of input and output called supervised learning and it has huge applications. Basically, we are trying to create a perfect function which goes through (x,y) coordinates we locked. Neural networks are very good at such tasks because of their neural plasticity or ability to quickly change their behavior by changing weights.\n",
    "This task is usually solved as follows:\n",
    "1. We declare **loss** function, which shows how good neural network in fitting into training examples\n",
    "2. Considering loss function as a function with several variables - weights, we calculate partial derivatives of loss function with respect to the weights\n",
    "3. We shift weights iteratively using vanilla gradient descent\n",
    "\n",
    "Task 1 is quite easy: loss function can be defined as $$\\mathcal{L}=\\frac{1}{2}\\sum_i(y_i(\\mathbf{\\hat{x}})-\\hat{y_i})^2$$where two lists of numerical values $(\\mathbf{\\hat{x},\\hat{y}})$ is a training example and summation is done by each output layer neuron.\n",
    "\n",
    "Task 3 was discussed in the previous lecture and can be described in one string: $$w^{t+1}=w^t-\\frac{\\partial\\mathcal{L}}{\\partial w}$$\n",
    "\n",
    "Task 2 is solvable using partial derivative but can be quite hard for understanding. Its solution is an algorithm called backpropagation and its derivation is going to be written for an interested reader into separate pdf file, which is going to be in the same github directory as this Jupyter notebook. Nevertheless, we are going to fully use its fruits in the following code, which is final iteration of NeuralNet class, which should be placed in separate library 'neural_net_lib'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet:\n",
    "    def __init__(self,inp,hid,out,epsilon):\n",
    "        #initial parameters are numbers of neurons in input (inp), hidden (hid), and output (out) layers \n",
    "        #plus epsilon value for weight initialization\n",
    "        #firstly, write down numbers of neurons in each layer \n",
    "        self.inp=inp\n",
    "        self.hid=hid\n",
    "        self.out=out\n",
    "        #secondly, create weights\n",
    "        #wa - weights of connection between input and hidden layers\n",
    "        #wb - weights of connection between hidden and output layers\n",
    "        import random\n",
    "        self.wa=[]\n",
    "        self.wb=[]\n",
    "        \n",
    "        #fill weight tables with random values from -epsilon to epsilon\n",
    "        for i in range(self.inp):\n",
    "            weights_row=[] \n",
    "            for j in range(self.hid):\n",
    "                weights_row.append(random.uniform(-epsilon,epsilon)) \n",
    "            self.wa.append(weights_row)  \n",
    "            \n",
    "        for i in range(self.hid):\n",
    "            weights_row=[] \n",
    "            for j in range(self.out):\n",
    "                weights_row.append(random.uniform(-epsilon,epsilon)) \n",
    "            self.wb.append(weights_row) \n",
    "            \n",
    "    #define activation function\n",
    "    def sigmoid(self,x):\n",
    "        return 1/(1+math.exp(-x))\n",
    "    \n",
    "    #calculate y output\n",
    "    def y(self,x):\n",
    "        \n",
    "        #firstly, calculate outputs of hidden layer neurons\n",
    "        hidden_values=[]\n",
    "        for j in range(self.hid):    \n",
    "            yj=0\n",
    "            for i in range(self.inp):\n",
    "                yj+=x[i]*self.wa[i][j] #i - number of input layer neuron , j - number of hidden layer neuron\n",
    "\n",
    "            yj=self.sigmoid(yj)\n",
    "            hidden_values.append(yj)\n",
    "\n",
    "        #secondly, hidden layer is treated as an input layer\n",
    "        output_values=[]\n",
    "        for k in range(self.out):\n",
    "            zk=0\n",
    "            for j in range(self.hid):\n",
    "                zk+=hidden_values[j]*self.wb[j][k] #j - number of hidden layer neuron , k - number of output layer neuron\n",
    "\n",
    "            zk=self.sigmoid(zk)\n",
    "            output_values.append(zk)\n",
    "            \n",
    "        return output_values\n",
    "    \n",
    "    def train(self,x_data,y_data,eta):\n",
    "        for t in range(len(x_data)):\n",
    "            #firstly, calculate outputs of hidden layer neurons\n",
    "            hidden_values=[]            \n",
    "            for j in range(self.hid):    \n",
    "                yj=0\n",
    "                for i in range(self.inp):\n",
    "                    yj+=x_data[t][i]*self.wa[i][j]#i - number of input layer neuron , j - number of hidden layer neuron\n",
    "\n",
    "                yj=self.sigmoid(yj)\n",
    "                hidden_values.append(yj)\n",
    "\n",
    "            #secondly, hidden layer is treated as an input layer\n",
    "            output_values=[]            \n",
    "            for k in range(self.out):\n",
    "                zk=0\n",
    "                for j in range(self.hid):\n",
    "                    zk+=hidden_values[j]*self.wb[j][k] #j - number of hidden layer neuron , k - number of output layer neuron\n",
    "\n",
    "                zk=self.sigmoid(zk)\n",
    "                output_values.append(zk)\n",
    "            \n",
    "            delta_k=[]\n",
    "            for k in range(self.out):\n",
    "                delta_k.append((output_values[k]-y_data[t][k])*output_values[k]*(1-output_values[k]))\n",
    "                               \n",
    "            delta_j=[]\n",
    "            for j in range(self.hid):\n",
    "                s=0\n",
    "                for k in range(self.out):\n",
    "                    s+=delta_k[k]*self.wb[j][k]\n",
    "                delta_j.append(s*hidden_values[j]*(1-hidden_values[j]))\n",
    "                               \n",
    "            for j in range(self.hid):\n",
    "                for k in range(self.out):\n",
    "                    self.wb[j][k]-=eta*delta_k[k]*hidden_values[j]\n",
    "                               \n",
    "            for i in range(self.inp):\n",
    "                for j in range(self.hid):\n",
    "                    self.wa[i][j]-=eta*delta_j[j]*x_data[t][i]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, we are going to test power of neural networks on finding patterns in logic tables which represent logic operators like AND, OR and XOR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#this example shows how new function train works on XOR logic table\n",
    "#x1 x2 y1\n",
    "#0  0  0\n",
    "#0  1  1\n",
    "#1  0  1\n",
    "#1  1  0\n",
    "nn=NeuralNet(2,5,1,0.1)\n",
    "x_data=[[0,0],[0,1],[1,0],[1,1]]\n",
    "y_data=[[0],[1],[1],[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train neural net iteratively with learning rate eta=1\n",
    "eta=1\n",
    "for i in range(10001):\n",
    "    #each 1000 iterations print side by side actual neural net result and desired y value\n",
    "    if i%1000==0:\n",
    "        print()\n",
    "        for k in range(len(x_data)):\n",
    "            print(nn.y(x_data[k]),y_data[k])          \n",
    "    \n",
    "    nn.train(x_data,y_data,eta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works! After several thousands iterations neural net shows very close results to a desired one!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Implement neural network **without** hidden layers from a scratch as a class \"NeuralNet0\" and save it in 'neural_net_lib'\n",
    "2. Train neural network \"NeuralNet\" on AND,OR and XOR logic gates. Try different numbers of hidden neurons. **Remember!** Size of x-value and y-value lists must be the same as number of input and output neurons, respectively.\n",
    "3. Repeat task 2 for \"NeuralNet0\"\n",
    "4. Compare typical values of loss functions from task 2 and 3. Which neural network is better?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
